{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import string\n",
    "import datetime as dt\n",
    "import pathlib\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import string\n",
    "import re\n",
    "from pattern.text.en import singularize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()\n",
    "os.chdir(\"./Mike Hulett/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text_file(file_path):\n",
    "\twith open(file_path, 'r', encoding='utf-8' ) as f:\n",
    "\t# with open(file_path, 'r', encoding='cp1252' ) as f:\n",
    "\t\treturn f.read()\n",
    "\t\t\n",
    "def print_punctuation(input_string=None):\n",
    "\tpunct_str = \"\"\n",
    "\tfor i in range(len(input_string)):\n",
    "\t\tchar = input_string[i] \n",
    "\t\tchar = char.replace('”', \"\\\"\")\n",
    "\t\tchar = char.replace('“', \"\\\"\")\n",
    "\t\tchar = char.replace(\"\\n\", \" \")\n",
    "\t\tif char in string.punctuation:\n",
    "\t\t\tpunct_str = punct_str + char\n",
    "\treturn punct_str\n",
    "\n",
    "def text_lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "def remove_numbers(text):\n",
    "    result = re.sub(r'\\d+', '', text)\n",
    "    return result\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(translator)    \n",
    "\n",
    "def remove_whitespace(text):\n",
    "    return  \" \".join(text.split())\n",
    "\n",
    "def count_sentences(text):\n",
    "\treturn len(sent_tokenize(text))\n",
    "\n",
    "def count_words(text):\n",
    "    return len(word_tokenize(text))\n",
    "    # word_count = [n+1 for word in word_tokens]\n",
    "\n",
    "def count_distinct_words(text):\n",
    "    return len(set(word_tokenize(text)))\n",
    "\n",
    "def build_vocab(existing_list, new_text, distinct=True):\n",
    "    new_words = word_tokenize(new_text)\n",
    "    existing_list.extend(new_words)\n",
    "    if distinct==True:\n",
    "        return list(set(existing_list))\n",
    "    else:\n",
    "        return(list(existing_list))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_text = [word for word in word_tokens if word not in stop_words]\n",
    "    return filtered_text\n",
    "\n",
    "def clean_string(text):\n",
    "    text = text.replace('”', \"\\\"\")\n",
    "    text = text.replace('“', \"\\\"\")\n",
    "    text = text.replace('’', \"\\'\")\n",
    "    text = text_lowercase(text)\n",
    "    text = remove_numbers(text)\n",
    "    text = remove_punctuation(text)\n",
    "    text = remove_whitespace(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = ' '.join([str(elem) for elem in text])\n",
    "    return text\t\n",
    "\n",
    "def clean_incl_stopwords(text):\n",
    "    text = text.replace('”', \"\\\"\")\n",
    "    text = text.replace('“', \"\\\"\")\n",
    "    text = text.replace('’', \"\\'\")\n",
    "    text = text_lowercase(text)\n",
    "    text = remove_numbers(text)\n",
    "    text = remove_punctuation(text)\n",
    "    text = remove_whitespace(text)\n",
    "    # text = remove_stopwords(text)\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_text = [word for word in word_tokens]\n",
    "    text = ' '.join([str(elem) for elem in filtered_text])\n",
    "    return text\t    \n",
    "\n",
    "def vocab_freq(word_list,\n",
    "               _singularize=False,\n",
    "               top_n=None):\n",
    "\n",
    "    _dict = {\n",
    "        'word':[],\n",
    "        'freq':[]\n",
    "    }\n",
    "\n",
    "    if _singularize == True:\n",
    "        singles = [singularize(plural) for plural in word_list]\n",
    "        distinct = list(set(singles))\n",
    "        for d in range(len(distinct)):\n",
    "            word = distinct[d]\n",
    "            freq = singles.count(word)\n",
    "            _dict['word'].append(word)\n",
    "            _dict['freq'].append(freq)\n",
    "    else:\n",
    "        distinct = list(set(word_list))\n",
    "        for d in range(len(distinct)):\n",
    "            word = distinct[d]\n",
    "            freq = word_list.count(word)\n",
    "            _dict['word'].append(word)\n",
    "            _dict['freq'].append(freq)\n",
    "\n",
    "    df = pd.DataFrame(_dict)\n",
    "    df.sort_values(by='freq', ascending=False, inplace=True)\n",
    "    df.set_index('word', inplace=True)\n",
    "    \n",
    "    if top_n:\n",
    "        return df.head(top_n)\n",
    "    else:\n",
    "        return df\n",
    "\n",
    "def stack_string(text, max_length=25):\n",
    "    stacked = ''\n",
    "    n = 0\n",
    "    for i in range(len(text)):\n",
    "        stacked = stacked + text[i]\n",
    "        n+=1\n",
    "        if n == max_length:\n",
    "            stacked = stacked + '\\n'\n",
    "            n = 0\n",
    "    return stacked + '\\n'    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd = os.getcwd()\n",
    "file_list = os.listdir()\n",
    "columnist = wd.split('\\\\')[len(wd.split('\\\\'))-1:][0]\n",
    "\n",
    "# file names are in YYYYMMDD.txt format\n",
    "format = '%Y%m%d'\n",
    "\n",
    "dict = {\n",
    "\t\"columnist\":[],\n",
    "\t\"file_date\":[],\n",
    "\t\"story_punctuation\":[],\n",
    "\t\"word_count\":[],\n",
    "\t\"distinct_word_count\":[],\n",
    "\t\"sentence_count\":[]\n",
    "}\n",
    "\n",
    "dict_vocab = {\n",
    "\t\"columnist\":[],\n",
    "\t\"distinct_words\":[],\n",
    "\t\"distinct_words_excl_stop\":[],\n",
    "\t\"all_nonstop_words\":[]\n",
    "}\n",
    "\n",
    "vocab_w_stop = []\n",
    "vocab_no_stop = []\n",
    "all_words = []\n",
    "\n",
    "# iterate through all files\n",
    "for i in range(len(file_list)):\n",
    "\tfile = file_list[i]\n",
    "\tymd = file[:8]\n",
    "\n",
    "\t# Check whether file is in text format or not\n",
    "\tif file.endswith(\".txt\"):\n",
    "\t\tfile_path = f\"{wd}\\{file}\"\n",
    "\t\tfile_date = dt.datetime.strptime(ymd, format)\n",
    "\t\ts = read_text_file(file_path)\n",
    "\t\tcleaned_string = clean_string(s)\n",
    "\t\tclean_with_stopwords = clean_incl_stopwords(s)\n",
    "\t\tp = print_punctuation(s)\n",
    "\t\tvocab_no_stop = build_vocab(vocab_no_stop, cleaned_string)\n",
    "\t\tvocab_w_stop = build_vocab(vocab_w_stop, clean_with_stopwords)\n",
    "\t\tall_words = build_vocab(all_words, cleaned_string, distinct=False)\n",
    "\n",
    "\t\tdict[\"columnist\"].append(columnist)\n",
    "\t\tdict[\"file_date\"].append(file_date)\n",
    "\t\tdict[\"story_punctuation\"].append(p)\n",
    "\t\tdict[\"word_count\"].append(count_words(s))\n",
    "\t\tdict[\"distinct_word_count\"].append(count_distinct_words(s))\n",
    "\t\tdict[\"sentence_count\"].append(count_sentences(s))\n",
    "\n",
    "dict_vocab[\"columnist\"].append(columnist)\n",
    "dict_vocab[\"distinct_words\"].append(vocab_w_stop)\n",
    "dict_vocab[\"distinct_words_excl_stop\"].append(vocab_no_stop)\n",
    "dict_vocab[\"all_nonstop_words\"].append(all_words)\n",
    "\n",
    "df = pd.DataFrame(dict)\n",
    "df_vocab = pd.DataFrame(dict_vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hl = df_vocab.all_nonstop_words.to_list()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>biden</th>\n",
       "      <td>225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trump</th>\n",
       "      <td>176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>american</th>\n",
       "      <td>147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>president</th>\n",
       "      <td>136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>democrat</th>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>america</th>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>white</th>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>person</th>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>would</th>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>state</th>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           freq\n",
       "word           \n",
       "biden       225\n",
       "trump       176\n",
       "american    147\n",
       "president   136\n",
       "democrat    122\n",
       "america     101\n",
       "white        83\n",
       "person       81\n",
       "would        79\n",
       "state        76"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = vocab_freq(hl, _singularize=True, top_n=10)\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>columnist</th>\n",
       "      <th>file_date</th>\n",
       "      <th>story_punctuation</th>\n",
       "      <th>word_count</th>\n",
       "      <th>distinct_word_count</th>\n",
       "      <th>sentence_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mike Hulett</td>\n",
       "      <td>2020-04-11</td>\n",
       "      <td>,.\",\".,-,.,.,-.,,',-.,-.,,-..,.,,.,,\".\",.\".\",,...</td>\n",
       "      <td>631</td>\n",
       "      <td>374</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mike Hulett</td>\n",
       "      <td>2020-04-18</td>\n",
       "      <td>,:\",,.,..\"!..,,,.:\",,.\".,.:\"(),.\".:\",,--?\"?:\",...</td>\n",
       "      <td>607</td>\n",
       "      <td>321</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mike Hulett</td>\n",
       "      <td>2020-05-09</td>\n",
       "      <td>;,..!,./.,,.%..:,,,,,,;.,,,..,....!.,....,,.,....</td>\n",
       "      <td>585</td>\n",
       "      <td>314</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mike Hulett</td>\n",
       "      <td>2020-05-23</td>\n",
       "      <td>-.,,....,-.,,-.,.-.-.\",\"-?.-\"\".-\"\",-..\".\"-,\"\"....</td>\n",
       "      <td>616</td>\n",
       "      <td>361</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mike Hulett</td>\n",
       "      <td>2020-06-20</td>\n",
       "      <td>.,??..,:,,,-?:\",,.\"??,?,.\"\".\"\".,,.,,.,.,,-..:,...</td>\n",
       "      <td>591</td>\n",
       "      <td>303</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>Mike Hulett</td>\n",
       "      <td>2022-09-24</td>\n",
       "      <td>,\",.\"!..:\"-..\",,,.:\":?-?\",\",,,,-..,,.\",\"\",-\"-\"...</td>\n",
       "      <td>599</td>\n",
       "      <td>309</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>Mike Hulett</td>\n",
       "      <td>2022-10-08</td>\n",
       "      <td>\"?\",.,.,-.--..-\",\",\",\".-:\"..\",!,:\"..,,....\":\"?...</td>\n",
       "      <td>587</td>\n",
       "      <td>337</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>Mike Hulett</td>\n",
       "      <td>2022-10-22</td>\n",
       "      <td>,.,.-,,,\".\",,.,.,,%,\"\".%.\"-,.\".\";.\".\".\",\"\".,-?...</td>\n",
       "      <td>572</td>\n",
       "      <td>320</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>Mike Hulett</td>\n",
       "      <td>2022-11-05</td>\n",
       "      <td>,:,-,.,.,,,-.\"-\",.,.-.\"\".!,.,--,-,,--,.-,,..-....</td>\n",
       "      <td>583</td>\n",
       "      <td>349</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>Mike Hulett</td>\n",
       "      <td>2022-11-19</td>\n",
       "      <td>.,.,,.-(!),..-.,,.:\",-,.\",..\"?\"(-:,,,,,-,!)\"-\"...</td>\n",
       "      <td>589</td>\n",
       "      <td>336</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>67 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      columnist  file_date                                  story_punctuation  \\\n",
       "0   Mike Hulett 2020-04-11  ,.\",\".,-,.,.,-.,,',-.,-.,,-..,.,,.,,\".\",.\".\",,...   \n",
       "1   Mike Hulett 2020-04-18  ,:\",,.,..\"!..,,,.:\",,.\".,.:\"(),.\".:\",,--?\"?:\",...   \n",
       "2   Mike Hulett 2020-05-09  ;,..!,./.,,.%..:,,,,,,;.,,,..,....!.,....,,.,....   \n",
       "3   Mike Hulett 2020-05-23  -.,,....,-.,,-.,.-.-.\",\"-?.-\"\".-\"\",-..\".\"-,\"\"....   \n",
       "4   Mike Hulett 2020-06-20  .,??..,:,,,-?:\",,.\"??,?,.\"\".\"\".,,.,,.,.,,-..:,...   \n",
       "..          ...        ...                                                ...   \n",
       "62  Mike Hulett 2022-09-24  ,\",.\"!..:\"-..\",,,.:\":?-?\",\",,,,-..,,.\",\"\",-\"-\"...   \n",
       "63  Mike Hulett 2022-10-08  \"?\",.,.,-.--..-\",\",\",\".-:\"..\",!,:\"..,,....\":\"?...   \n",
       "64  Mike Hulett 2022-10-22  ,.,.-,,,\".\",,.,.,,%,\"\".%.\"-,.\".\";.\".\".\",\"\".,-?...   \n",
       "65  Mike Hulett 2022-11-05  ,:,-,.,.,,,-.\"-\",.,.-.\"\".!,.,--,-,,--,.-,,..-....   \n",
       "66  Mike Hulett 2022-11-19  .,.,,.-(!),..-.,,.:\",-,.\",..\"?\"(-:,,,,,-,!)\"-\"...   \n",
       "\n",
       "    word_count  distinct_word_count  sentence_count  \n",
       "0          631                  374              28  \n",
       "1          607                  321              29  \n",
       "2          585                  314              33  \n",
       "3          616                  361              30  \n",
       "4          591                  303              31  \n",
       "..         ...                  ...             ...  \n",
       "62         599                  309              24  \n",
       "63         587                  337              20  \n",
       "64         572                  320              18  \n",
       "65         583                  349              27  \n",
       "66         589                  336              21  \n",
       "\n",
       "[67 rows x 6 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ",.\",\".,-,.,.,-.,,',-.,-.,,-..,.,,.,,\".\",.\".\",,-.,-\n",
      "..,?,,.!-,,?,..,.,.,\",\"..,,.,..:\",,.\"?..,,.\"\":\":..\n",
      "..\";!\n",
      "\n",
      ",:\",,.,..\"!..,,,.:\",,.\".,.:\"(),.\".:\",,--?\"?:\",,,?\"\n",
      ",.,.,.\"\".,.\",\"..-\".\".,-...,..,-:\"...-.\"!,-\"\"..\n",
      "\n",
      ";,..!,./.,,.%..:,,,,,,;.,,,..,....!.,....,,.,..,,,\n",
      ".-..,,,.....,,.\n",
      "\n",
      "-.,,....,-.,,-.,.-.-.\",\"-?.-\"\".-\"\",-..\".\"-,\"\".,:\"?\n",
      "\":\".\",.?-?:.\"\"..,.,.,,,,!.-?:--,\".\",,.,.-,-,.,.().\n",
      "..,.\n",
      "\n",
      ".,??..,:,,,-?:\",,.\"??,?,.\"\".\"\".,,.,,.,.,,-..:,,.(%\n",
      ")-..,,.,-,(),.(,.)..(%),,,:(%).\"\".,.:,,--+.,-.\"\".\n",
      "\n",
      "...,.,,.\"\",,.,,,,.,.,.,-,.,.;....,..,.\"\".\"\",..,.,,\n",
      ",.,-.,\".\",,.,.,,..,.-,-,.:,,?,,\"\"\"\"?.,,?\n",
      "\n",
      ".,/...,...,.-.,-.??.,.,.?,,.:.......,..?..:\",,.\"--\n",
      ",..-,..\",\",.\"\".,:??,--?..\n",
      "\n",
      ".,,..,.......\",\",,..,..,..\"\"..\",\",,,,...,....,-,,-\n",
      "\".\"\".,.\":.\",\"-,,.,,\".\"-,.:,.-,\",\",-,.\n",
      "\n",
      "--....,.:,,,,,,,.,.,,\"\".,.,,.(--),,,,,,,,,,....,.-\n",
      ",?!,.?\"!\"!-\"\".,.,\"\"\"-.\",,.,.,.,\".\":,,.\",\",..\",\"\".\"\n",
      ".!\n",
      "\n",
      ":,\"\"\"\".--,,,.--,.\"\".,,\",,,.\":\".\",..,.\"\";-.,.?.,--,\n",
      ".,-.\"\",\".\":\"..\",,.,-,,,,\"\",!-.-,,,,\"\".-,-\".\",-.\n",
      "\n",
      ".,-.,.,,.\".\"\"-.\",.,,.,,.'.,.,,..,.,%,,.,-.,.,-..,.\n",
      ":\".\",.--?..-,...,\"\".,....,.,\"!\",-:\",.,,..?,,,,?.\"\n",
      "\n",
      "\",,.\"-.,:\",.\",,\"?!,,,.,.\"....,,,.....,,,$.%..,.,.,\n",
      ",.,,$.,,.,,.-..,,,..,,-.,,-,.,\".\"\n",
      "\n",
      "-:?.,,.\",.\",.-..,\".\"-\",\"..:\".,...,-,.\":\"---...,.\"-\n",
      ",,.\".\".,:\".\",:\",:..\",,.?,,.\"\"..?:,\".\".,-..,,,,!\n",
      "\n",
      ",/,.::\",..\"-,,.,,,..,,..\"?\"\"\"\"\"?,-:\",,\"\".\":-...,,\"\n",
      "\"\"\"?!\"?\"\"\",....:---,,-,.!,,,,,,.,.,.-..\n",
      "\n",
      "\"\",,,-.,,.\"\"-.:\".\",,,.!,:\"-!!\",:.,...,,.,-\"\"-,,.\"\"\n",
      "..,...-.,,--,-.,:\"!\",,..,.\"\".\n",
      "\n",
      "-,/,.\"..!\"!,,,.\",!\"-\"\".\"\",,...,-.,,-.,''.-.,.-.,,.\n",
      ".-\"\"?:.:\"-!\"\"\",,,.\"\".,\"\"-..,....!%,,..--...,,.%.?.\n",
      ",?.\n",
      "\n",
      ".,,,.,.:\",.,:,.-,,.,.?\"!--:\"...\"!,.,,,-.,\"\"-..,.,.\n",
      "\".\"-,,,:\"().\"!?\"..\"..,\".\",\".\"!;\"\"\".\"!:.\"\".,\"\"\"\".,.\n",
      ".,,,,,..\n",
      "\n",
      ":\".\":\"!\"!.\"\"',,,,..()..,.,,....()\"\"\".\"()\"\".;.()-..\n",
      ".;.(),-.,\".\"():\".\"()',,,,.(,)\"\",.(,)/.(,)!(,)-,,,.\n",
      "..-,,,,----.,,\"\"\",\".:-.--.,,,,,!\n",
      "\n",
      "\"----\".\",,.,.\":,-,.,-,,.!...,.,.,.,-.,,.,-..,....;\n",
      ".:\",,,.\".,\".\"()-,-,.,,,.(,)\",\"%...-,%.,%\"\".\",\".\"\",\n",
      ",;,,....\n",
      "\n",
      "-,,,..,.,.,.--..,.?,.-.(.)...-..:.:,,.!.!-.,..\"\"..\n",
      "...,..,...\".\",.\n",
      "\n",
      ":\",,().,...()-.,,.\"(\",\".\"\".)\",\",.-.\"\",,,,\"...\",,!-\n",
      "\"\"-,.,.\"-\",\"..,\"?:\",??--,.\",:\".\"?--./.-/.,,/./.\"?\"\n",
      "\"\"-?..\".\"\n",
      "\n",
      "\",\".\"','..\"'.\"\"?,-.-,.-\"\"--.,.\"\"-:...-..-..,..-.\"\"\n",
      "-:-......-,./.,\".\"..-,,...\"\"...,..\".\"-\"\"-,.\"!\"\n",
      "\n",
      "&\"\"-..:\"\"\"-.\",..,....,.,,.$,.-.?-/.,.,-.,,,-....!.\n",
      ",!-..,,,\"!\"\n",
      "\n",
      ".-,,\"\".-,,,..-..\"\"\"\",\".\"\",\"\"\"!-,,\"\"--\"\"\"!\",,\"\".\"\"\"\n",
      ",\"\"\".\"\",\"\"\"\"-.\".\"\"\",.-,!,/.\",\".,.-..\"\".,,-.\n",
      "\n",
      "...-,.-\",\",.\"\"\"\".\"\".\"\"\",,.\"\"\",.\".\"-,,,-,(%),(%).,-\n",
      "\".\",,-..,%.--.,.,,.,,,%,.(%)\".\".,-,(),.,,\".\":\"\".,.\n",
      ",,.,,\"\".\n",
      "\n",
      "...,\",\".\"\".,.,.,-..-,,,...,:),,)..\"\".,.\".\",,\"\".,,,\n",
      "??.:-:\";,..,;,.\"\n",
      "\n",
      ".-,.,,.,.....,-,,.-.\"\"().-,\"\".-.,.,....,.,,-,,,,.-\n",
      ",+,.-..,.,,......,,\"\"....,.,.\n",
      "\n",
      ",-,,,:\"..\":\".\"\"\"\"\"\"\"\".\",-.,\",\".,,..,,:\".\",\"\"\"\".-.,\n",
      ",.,.,:\".,,;.\",\".\".,.-,,,-,,-,-.\"\",-.-,-,-,\"\"-,,,,.\n",
      "\",\"\"\"-,.\n",
      "\n",
      "!.....,.\"\"/-,,,.,-..-,-.,,.,,,,.(.)...,.+-.,...$,,\n",
      "$,.-.,,-.\"\"..:,,,,,,,,,,,,,(,,),,-,!:,,.,\"\"\".\".,,,\n",
      ",-...!\n",
      "\n",
      "\",\"-,:\"',.\"\"\"....\".\".,,.\"\"\"\"-.,..,\".\",,,\"\".\",,\",,\"\n",
      "\".--,-..\".\",..\",\"\".\"-..,,-.,\".\",,!\",\"\n",
      "\n",
      "..,,().!\"\".,,:\",,.',,..\".,,.,\"\".,-,:\",,,,,,,..,,.\"\n",
      ".,,,\"\"\".\",,.,-.,.\"\"??\".\",,,,,.\n",
      "\n",
      ",,.,,.,,..,,,,,,,\"-!\".,,,.,...,.,,..,,.,,..,',.,..\n",
      ".,.,-.,.,,.,-..,.,,..,--\"\".--.\n",
      "\n",
      ":\"\",\"\",\"\",\"\",\".\":.--.%%.-:..:,.:%%.,!--:..-.-,,.-.\n",
      ".,,.!,.\"\"\".\":\"'.\"(.).,--,,,-.\"\"\".\"...\"\"..(-).\",!\"?\n",
      "?,,\".\"!\n",
      "\n",
      ",,,./.--.,,.,-.,,.,,,,.-...,.,.(.)-,,-,,-.,,..,,.,\n",
      ".,...,-.,,,,.;.,\"\",,.,,,-,.,,,,.,.,,.\n",
      "\n",
      ".,-,......,:\".\",,,,,....,..,--..,\"--,\".,,.,,....,;\n",
      ".:,?,()...,-...,.\n",
      "\n",
      ",,,.\".\"\"\"\"\"-.:\"',''.\",!'..,,,-...\"\"%%,%%..,......,\n",
      ",,...?,,-.,.\"\".!,.,-./-?..!.,!,\".\"\n",
      "\n",
      ".,.,.\"\"-\"-.\",-,,,.\"\".\"\"\"\".\"\"?,-.-.,./,..-..,.,,..,\n",
      ".,,.--.,\".\",,.---,,../,-.,,.,.\n",
      "\n",
      ".\"\",..,...,..,....,,.?,-.-?-?-.\"\"...,\",\".,-.$+....\n",
      "\"\"\"\",--,-!?.,.?,.,-,.-,,,,...-...,.\n",
      "\n",
      "\"\"...\"\"\".\"\"\",,\".\",.,\"\"\".\",..'\"\".,.,\",\"\",\"\"\"...\"\"\",\n",
      ",-.\",\"\"\"\".\"-\",\".\"-..,\":,\".,,..\"\"..\".,,,-,,,\".\",,.\"\n",
      "\n",
      "\n",
      "....,..,..,.,,\".\".\",\"'.\"..\"\"-..,,,\".\".,,\"'.,,.,..$\n",
      ".,..',..,....:??,,??,!\n",
      "\n",
      ",:\".\"\",.,.,,.,.\".,,.-,..,.,.-.:,.\"\",.,\",\".\",\"\".\".-\n",
      ".,,-,.\".\":\"-.\".-..,:\"\".-.-.\".\",,.\n",
      "\n",
      ",...-.,.,,\"\"\".\"...,.,\"\"\".\"-.,,,\"\"..--,,..,..,,.,.,\n",
      "\".\"\"\"\".\"-....-.-,,,-.\n",
      "\n",
      ",...-.,.,,\"\"\".\"...,.,\"\"\".\"-.,,,\"\"..--,,..,..,,.,.,\n",
      "\".\"\"\"\".\"-....-.-,,,-.\n",
      "\n",
      "\"\"--.,-,,.!,.-,,.\":,,\"\":,,\"-.-.,.,,-!,,,,,,.,-,,-.\n",
      "..:?,,'-.,\"-\"...?,,,,,.\n",
      "\n",
      ".-.,,....,..,.,-.,./.?,.,-.,,.-...,-.(---)?,..,,,,\n",
      "-.!,-,%-,--,-,,.,\".\",.....,..,,--.\n",
      "\n",
      "..\",\".,.,,.,/,.-.,..\".\",-.,/,.,,.-..,..\"\"\"\"\"\".,\"\"!\n",
      "(!).,,.,.--,.,-,..?,,.\"\"?,,,.\"?\",.,?\n",
      "\n",
      ",,...,,,--...-.-.$.-,..,,,.,,..%.%,,%.,,.,..-,.--.\n",
      "\"\".\".\"..-?.,,.\n",
      "\n",
      "\"-,.,.\":\",..,..,..-.\".()--.,..-,,.:\"..?.\".\",\"..().\n",
      "\"\"\".\",.\"\"$..,,.,,-..---..:\"-...,-,!\"-,,,,,,,-.,...\n",
      ",,.\n",
      "\n",
      "\".\"\"\"\"\"\".\",\"\"\"\".\"\"..!,--..,,,,,,.,,.-,,,,,..-..,,.\n",
      "..,,-,...$.\"\".,....---..,-?\"!\"\n",
      "\n",
      ",..-,,,.,..---..,-..,.-,,..,,..\"\",-.,.,,,.,.,.\"\".,\n",
      "'.,.\"\"......,.,.,.,,?\n",
      "\n",
      "....,(,,,),.,.,,,,.,.,,,.(:\"\"\"!\"),-.,:\",-.\"...,..,\n",
      ".,-.,\"--\".\"\".,\"\"...,\"\",,.,,.,,\",\",,.\"\".\n",
      "\n",
      ",,\"\".-....,,:\",...,():.\":\".,,-,.-,-...,.\"-\"\",..,,.\n",
      "...,\".\"-..(\"\").\"\".%.%'-,.%,(),..-,..-.,,,,.\n",
      "\n",
      ",,.-:\",.-,--.\":\",.\"..,,,,,\"\".:\",,.,..\",\",\"\",\"\",\".,\n",
      ",()$,().'.,.,..,.,,\".,.\",,\"\"\".\".\",\"..\n",
      "\n",
      "\"\",...,.---,,...\"\"\",\"\"\"\",\".\",\",,..\"\",,,.:\",.\",,\"\"\"\n",
      "?\"-\".\":\".\"..,,..,,,,,,.,,.\":.%:.\"'.\"\":\",\"-,.\n",
      "\n",
      "..(,,,,,).-.:\",,-.\"\"\",.\"\",\"\"!...-...\"\"-..,-.,.,,,.\n",
      "\",\",,.,.,.'-.,/,.\"\".,-\".\".\"\"..,,\"\"\"!\",-,,.,?\n",
      "\n",
      ",-.\":..\"..,.,..,,,..,-,.,-.-.-...,.,.,..-,....,,,,\n",
      ".,..\n",
      "\n",
      "-,.\"\"\".\"....\"\".,,-.,,-.\"\"(!).,,,,,,,,,-.,,..,..,\"&\n",
      ",\".,-.,,.,!,',..,:.\n",
      "\n",
      "%..\"\".-\",\"-..-.\"\"...,\",\".-.,---.-..,.\".,\"..-...,,.\n",
      ".\"\".-,,...:,,,,,,-;,-.,-.,.,.\".\"(.)!.\"\"!\n",
      "\n",
      "-:\".\",.-..,.-,.,\"\",,..,.,..,.-,?..,,\"--\",,-.,-,.,/\n",
      "-.,?,.,,-,-,--,,-.-..,\"\"\",,.\"\n",
      "\n",
      ".,,.-.,..,,..-?\"!\",,,.\",\".\",..\"(.!)..,-,...,\".-.\",\n",
      ",,-,.,,.-.,..,,,?.-.,-',..,?\n",
      "\n",
      ".-.-.,:\"!\",..,.,,\"',,,.\",..,\"\".,,,-,-.\"\"..,.\".\".-.\n",
      ",.,\".\"!,..,.!?!.!??,,...,,.,.!-...,---\".\"\n",
      "\n",
      ",\"\"\",,.\".,,.?,,,,;.\"-\",-,,.,,!?....,-.?--,.,,.--,-\n",
      ".?!,,,,-,..,.,-.-.-,.!--,.,,,-,-.,,,,.,\"%\".\"\"..,\",\n",
      ",,,.\"\n",
      "\n",
      ",\",.\"!..:\"-..\",,,.:\":?-?\",\",,,,-..,,.\",\"\",-\"-\",?..\n",
      ",.,\"\"-\"?\"\"\"\"\"\"\"?,,.,,.,,.,,-..\"\"\"\",,.,,.,,,.,,.!.\n",
      "\n",
      "\"?\",.,.,-.--..-\",\",\",\".-:\"..\",!,:\"..,,....\":\"?\".--\n",
      ".,,--.:\",.\":\"....\".,,,,.,,,-.\n",
      "\n",
      ",.,.-,,,\".\",,.,.,,%,\"\".%.\"-,.\".\";.\".\".\",\"\".,-?:\"..\n",
      ".\"...-,\"\"-.,,.,.\n",
      "\n",
      ",:,-,.,.,,,-.\"-\",.,.-.\"\".!,.,--,-,,--,.-,,..-..\"\".\n",
      ",-.-,,,.,.--..,\"\".-,.-.,-\"\"-,,-.,-,.,,.,:.,,.\n",
      "\n",
      ".,.,,.-(!),..-.,,.:\",-,.\",..\"?\"(-:,,,,,-,!)\"-\",--,\n",
      ",\"\"\"-.\",-.\"-\".,\"-\"--.,\"-\".,-.-.,..,,.,\"\"\".\"..,,.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(df)):\n",
    "    print(stack_string(df['story_punctuation'][i], 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg total word count:  590 StDev total word count: 15.5\n",
      "Avg distinct word count: 329 StDev distinct word count: 19.0\n"
     ]
    }
   ],
   "source": [
    "avg_word_count = np.floor(np.average(df.word_count)).astype(np.int32)\n",
    "avg_distinct_word_count = np.floor(np.average(df.distinct_word_count)).astype(np.int32)\n",
    "\n",
    "std_word_count = round(np.std(df.word_count),1)\n",
    "std_distinct_word_count = round(np.std(df.distinct_word_count),1)\n",
    "\n",
    "print('Avg total word count: ', avg_word_count, 'StDev total word count:', std_word_count)\n",
    "print('Avg distinct word count:', avg_distinct_word_count, 'StDev distinct word count:', std_distinct_word_count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7e4608ffab33544aecd2f97721d910c9b0c38b0c6fa6ab7667a80c134f821554"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
