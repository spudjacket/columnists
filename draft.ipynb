{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import string\n",
    "import datetime as dt\n",
    "import pathlib\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import string\n",
    "import re\n",
    "from pattern.text.en import singularize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()\n",
    "os.chdir(\"./Mike Hulett/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text_file(file_path):\n",
    "\twith open(file_path, 'r', encoding='utf-8' ) as f:\n",
    "\t# with open(file_path, 'r', encoding='cp1252' ) as f:\n",
    "\t\treturn f.read()\n",
    "\t\t\n",
    "def print_punctuation(input_string=None):\n",
    "\tpunct_str = \"\"\n",
    "\tfor i in range(len(input_string)):\n",
    "\t\tchar = input_string[i] \n",
    "\t\tchar = char.replace('”', \"\\\"\")\n",
    "\t\tchar = char.replace('“', \"\\\"\")\n",
    "\t\tchar = char.replace(\"\\n\", \" \")\n",
    "\t\tif char in string.punctuation:\n",
    "\t\t\tpunct_str = punct_str + char\n",
    "\treturn punct_str\n",
    "\n",
    "def text_lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "def remove_numbers(text):\n",
    "    result = re.sub(r'\\d+', '', text)\n",
    "    return result\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(translator)    \n",
    "\n",
    "def remove_whitespace(text):\n",
    "    return  \" \".join(text.split())\n",
    "\n",
    "def count_sentences(text):\n",
    "\treturn len(sent_tokenize(text))\n",
    "\n",
    "def count_words(text):\n",
    "    return len(word_tokenize(text))\n",
    "    # word_count = [n+1 for word in word_tokens]\n",
    "\n",
    "def count_distinct_words(text):\n",
    "    return len(set(word_tokenize(text)))\n",
    "\n",
    "def build_vocab(existing_list, new_text, distinct=True):\n",
    "    new_words = word_tokenize(new_text)\n",
    "    existing_list.extend(new_words)\n",
    "    if distinct==True:\n",
    "        return list(set(existing_list))\n",
    "    else:\n",
    "        return(list(existing_list))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_text = [word for word in word_tokens if word not in stop_words]\n",
    "    return filtered_text\n",
    "\n",
    "def clean_string(text):\n",
    "    text = text.replace('”', \"\\\"\")\n",
    "    text = text.replace('“', \"\\\"\")\n",
    "    text = text.replace('’', \"\\'\")\n",
    "    text = text_lowercase(text)\n",
    "    text = remove_numbers(text)\n",
    "    text = remove_punctuation(text)\n",
    "    text = remove_whitespace(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = ' '.join([str(elem) for elem in text])\n",
    "    return text\t\n",
    "\n",
    "def clean_incl_stopwords(text):\n",
    "    text = text.replace('”', \"\\\"\")\n",
    "    text = text.replace('“', \"\\\"\")\n",
    "    text = text.replace('’', \"\\'\")\n",
    "    text = text_lowercase(text)\n",
    "    text = remove_numbers(text)\n",
    "    text = remove_punctuation(text)\n",
    "    text = remove_whitespace(text)\n",
    "    # text = remove_stopwords(text)\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_text = [word for word in word_tokens]\n",
    "    text = ' '.join([str(elem) for elem in filtered_text])\n",
    "    return text\t    \n",
    "\n",
    "def vocab_freq(word_list,\n",
    "               _singularize=False,\n",
    "               top_n=None):\n",
    "\n",
    "    _dict = {\n",
    "        'word':[],\n",
    "        'freq':[]\n",
    "    }\n",
    "\n",
    "    if _singularize == True:\n",
    "        singles = [singularize(plural) for plural in word_list]\n",
    "        distinct = list(set(singles))\n",
    "        for d in range(len(distinct)):\n",
    "            word = distinct[d]\n",
    "            freq = singles.count(word)\n",
    "            _dict['word'].append(word)\n",
    "            _dict['freq'].append(freq)\n",
    "    else:\n",
    "        distinct = list(set(word_list))\n",
    "        for d in range(len(distinct)):\n",
    "            word = distinct[d]\n",
    "            freq = word_list.count(word)\n",
    "            _dict['word'].append(word)\n",
    "            _dict['freq'].append(freq)\n",
    "\n",
    "    df = pd.DataFrame(_dict)\n",
    "    df.sort_values(by='freq', ascending=False, inplace=True)\n",
    "    df.set_index('word', inplace=True)\n",
    "    \n",
    "    if top_n:\n",
    "        return df.head(top_n)\n",
    "    else:\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd = os.getcwd()\n",
    "file_list = os.listdir()\n",
    "columnist = wd.split('\\\\')[len(wd.split('\\\\'))-1:][0]\n",
    "\n",
    "# file names are in YYYYMMDD.txt format\n",
    "format = '%Y%m%d'\n",
    "\n",
    "dict = {\n",
    "\t\"columnist\":[],\n",
    "\t\"file_date\":[],\n",
    "\t\"story_punctuation\":[],\n",
    "\t\"word_count\":[],\n",
    "\t\"distinct_word_count\":[],\n",
    "\t\"sentence_count\":[]\n",
    "}\n",
    "\n",
    "dict_vocab = {\n",
    "\t\"columnist\":[],\n",
    "\t\"distinct_words\":[],\n",
    "\t\"distinct_words_excl_stop\":[],\n",
    "\t\"all_nonstop_words\":[]\n",
    "}\n",
    "\n",
    "vocab_w_stop = []\n",
    "vocab_no_stop = []\n",
    "all_words = []\n",
    "\n",
    "# iterate through all files\n",
    "for i in range(len(file_list)):\n",
    "\tfile = file_list[i]\n",
    "\tymd = file[:8]\n",
    "\n",
    "\t# Check whether file is in text format or not\n",
    "\tif file.endswith(\".txt\"):\n",
    "\t\tfile_path = f\"{wd}\\{file}\"\n",
    "\t\tfile_date = dt.datetime.strptime(ymd, format)\n",
    "\t\ts = read_text_file(file_path)\n",
    "\t\tcleaned_string = clean_string(s)\n",
    "\t\tclean_with_stopwords = clean_incl_stopwords(s)\n",
    "\t\tp = print_punctuation(s)\n",
    "\t\tvocab_no_stop = build_vocab(vocab_no_stop, cleaned_string)\n",
    "\t\tvocab_w_stop = build_vocab(vocab_w_stop, clean_with_stopwords)\n",
    "\t\tall_words = build_vocab(all_words, cleaned_string, distinct=False)\n",
    "\n",
    "\t\tdict[\"columnist\"].append(columnist)\n",
    "\t\tdict[\"file_date\"].append(file_date)\n",
    "\t\tdict[\"story_punctuation\"].append(p)\n",
    "\t\tdict[\"word_count\"].append(count_words(s))\n",
    "\t\tdict[\"distinct_word_count\"].append(count_distinct_words(s))\n",
    "\t\tdict[\"sentence_count\"].append(count_sentences(s))\n",
    "\n",
    "dict_vocab[\"columnist\"].append(columnist)\n",
    "dict_vocab[\"distinct_words\"].append(vocab_w_stop)\n",
    "dict_vocab[\"distinct_words_excl_stop\"].append(vocab_no_stop)\n",
    "dict_vocab[\"all_nonstop_words\"].append(all_words)\n",
    "\n",
    "df = pd.DataFrame(dict)\n",
    "df_vocab = pd.DataFrame(dict_vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "hl = df_vocab.all_nonstop_words.to_list()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>biden</th>\n",
       "      <td>225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trump</th>\n",
       "      <td>176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>american</th>\n",
       "      <td>147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>president</th>\n",
       "      <td>136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>democrat</th>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>america</th>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>white</th>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>person</th>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>would</th>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>state</th>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>border</th>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>polouse</th>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>medium</th>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>black</th>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>moorhead</th>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>one</th>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>u</th>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>election</th>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>family</th>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>law</th>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           freq\n",
       "word           \n",
       "biden       225\n",
       "trump       176\n",
       "american    147\n",
       "president   136\n",
       "democrat    122\n",
       "america     101\n",
       "white        83\n",
       "person       81\n",
       "would        79\n",
       "state        76\n",
       "border       75\n",
       "polouse      72\n",
       "medium       72\n",
       "black        68\n",
       "moorhead     67\n",
       "one          63\n",
       "u            58\n",
       "election     56\n",
       "family       56\n",
       "law          55"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = vocab_freq(hl, _singularize=True, top_n=20)\n",
    "v"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7e4608ffab33544aecd2f97721d910c9b0c38b0c6fa6ab7667a80c134f821554"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
