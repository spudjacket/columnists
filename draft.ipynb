{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import string\n",
    "import datetime as dt\n",
    "import pathlib\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import string\n",
    "import re\n",
    "from pattern.text.en import singularize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()\n",
    "os.chdir(\"./Mike Hulett/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text_file(file_path):\n",
    "\twith open(file_path, 'r', encoding='utf-8' ) as f:\n",
    "\t# with open(file_path, 'r', encoding='cp1252' ) as f:\n",
    "\t\treturn f.read()\n",
    "\t\t\n",
    "def print_punctuation(input_string=None):\n",
    "\tpunct_str = \"\"\n",
    "\tfor i in range(len(input_string)):\n",
    "\t\tchar = input_string[i] \n",
    "\t\tchar = char.replace('”', \"\\\"\")\n",
    "\t\tchar = char.replace('“', \"\\\"\")\n",
    "\t\tchar = char.replace(\"\\n\", \" \")\n",
    "\t\tif char in string.punctuation:\n",
    "\t\t\tpunct_str = punct_str + char\n",
    "\treturn punct_str\n",
    "\n",
    "def text_lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "def remove_numbers(text):\n",
    "    result = re.sub(r'\\d+', '', text)\n",
    "    return result\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(translator)    \n",
    "\n",
    "def remove_whitespace(text):\n",
    "    return  \" \".join(text.split())\n",
    "\n",
    "def count_sentences(text):\n",
    "\treturn len(sent_tokenize(text))\n",
    "\n",
    "def count_words(text):\n",
    "    return len(word_tokenize(text))\n",
    "    # word_count = [n+1 for word in word_tokens]\n",
    "\n",
    "def count_distinct_words(text):\n",
    "    return len(set(word_tokenize(text)))\n",
    "\n",
    "def build_vocab(existing_list, new_text, distinct=True):\n",
    "    new_words = word_tokenize(new_text)\n",
    "    existing_list.extend(new_words)\n",
    "    if distinct==True:\n",
    "        return list(set(existing_list))\n",
    "    else:\n",
    "        return(list(existing_list))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_text = [word for word in word_tokens if word not in stop_words]\n",
    "    return filtered_text\n",
    "\n",
    "def clean_string(text):\n",
    "    text = text.replace('”', \"\\\"\")\n",
    "    text = text.replace('“', \"\\\"\")\n",
    "    text = text.replace('’', \"\\'\")\n",
    "    # adding this to split hyptenated words into separate words\n",
    "    text = text.replace('-', \" \")\n",
    "    text = text_lowercase(text)\n",
    "    text = remove_numbers(text)\n",
    "    text = remove_punctuation(text)\n",
    "    text = remove_whitespace(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = ' '.join([str(elem) for elem in text])\n",
    "    return text\t\n",
    "\n",
    "def clean_incl_stopwords(text):\n",
    "    text = text.replace('”', \"\\\"\")\n",
    "    text = text.replace('“', \"\\\"\")\n",
    "    text = text.replace('’', \"\\'\")\n",
    "    # adding this to split hyptenated words into separate words\n",
    "    text = text.replace('-', \" \")\n",
    "    text = text_lowercase(text)\n",
    "    text = remove_numbers(text)\n",
    "    text = remove_punctuation(text)\n",
    "    text = remove_whitespace(text)\n",
    "    # text = remove_stopwords(text)\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_text = [word for word in word_tokens]\n",
    "    text = ' '.join([str(elem) for elem in filtered_text])\n",
    "    return text\t    \n",
    "\n",
    "def vocab_freq(word_list,\n",
    "               _singularize=False,\n",
    "               top_n=None):\n",
    "\n",
    "    _dict = {\n",
    "        'word':[],\n",
    "        'freq':[]\n",
    "    }\n",
    "\n",
    "    if _singularize == True:\n",
    "        singles = [singularize(plural) for plural in word_list]\n",
    "        distinct = list(set(singles))\n",
    "        for d in range(len(distinct)):\n",
    "            word = distinct[d]\n",
    "            freq = singles.count(word)\n",
    "            _dict['word'].append(word)\n",
    "            _dict['freq'].append(freq)\n",
    "    else:\n",
    "        distinct = list(set(word_list))\n",
    "        for d in range(len(distinct)):\n",
    "            word = distinct[d]\n",
    "            freq = word_list.count(word)\n",
    "            _dict['word'].append(word)\n",
    "            _dict['freq'].append(freq)\n",
    "\n",
    "    df = pd.DataFrame(_dict)\n",
    "    df.sort_values(by='freq', ascending=False, inplace=True)\n",
    "    df.set_index('word', inplace=True)\n",
    "    \n",
    "    if top_n:\n",
    "        return df.head(top_n)\n",
    "    else:\n",
    "        return df\n",
    "\n",
    "def stack_string(text, max_length=25):\n",
    "    stacked = ''\n",
    "    n = 0\n",
    "    for i in range(len(text)):\n",
    "        stacked = stacked + text[i]\n",
    "        n+=1\n",
    "        if n == max_length:\n",
    "            stacked = stacked + '\\n'\n",
    "            n = 0\n",
    "    return stacked + '\\n'    \n",
    "\n",
    "def count_quoted_words(text):\n",
    "    text = text.replace('”', \"\\\"\")\n",
    "    text = text.replace('“', \"\\\"\")\n",
    "    quoted_words = ''\n",
    "    start_quote = 0\n",
    "    mid_quote = 0\n",
    "    end_quote = 0\n",
    "    for char in range(len(text)):\n",
    "        if start_quote == 1 and end_quote == 1:\n",
    "            start_quote = 0\n",
    "            end_quote = 0\n",
    "\n",
    "        if text[char] == \"\\\"\" and start_quote == 0 and end_quote == 0:\n",
    "            start_quote = 1\n",
    "\n",
    "        if text[char] != \"\\\"\" and start_quote == 1 and end_quote == 0:\n",
    "            mid_quote = 1\n",
    "            quoted_words += text[char]\n",
    "\n",
    "        if text[char] == \"\\\"\" and start_quote == 1 and mid_quote == 1:\n",
    "            quoted_words += \" \"\n",
    "            mid_quote = 0\n",
    "            end_quote = 1\n",
    "\n",
    "    quoted_words = clean_incl_stopwords(quoted_words)\n",
    "    word_count = count_words(quoted_words)\n",
    "\n",
    "    return word_count\n",
    "\n",
    "def stats_quoted_words(text):\n",
    "    text = text.replace('”', \"\\\"\")\n",
    "    text = text.replace('“', \"\\\"\")\n",
    "    quoted_words = ''\n",
    "    start_quote = 0\n",
    "    mid_quote = 0\n",
    "    end_quote = 0\n",
    "    short_quotes = []\n",
    "    long_quotes = []\n",
    "    quote_list = []\n",
    "    quote_word_len_list = []\n",
    "    \n",
    "    dict = {\n",
    "        'quote_word_count':[],\n",
    "        'quote_count':[],\n",
    "        'count_short':[],\n",
    "        'count_long':[],\n",
    "        'avg_len_short':[],\n",
    "        'avg_len_long':[]\n",
    "    }\n",
    "\n",
    "    for char in range(len(text)):\n",
    "        if start_quote == 1 and end_quote == 1:\n",
    "            quoted_words = ''\n",
    "            start_quote = 0\n",
    "            end_quote = 0\n",
    "\n",
    "        if text[char] == \"\\\"\" and start_quote == 0 and end_quote == 0:\n",
    "            start_quote = 1\n",
    "\n",
    "        if text[char] != \"\\\"\" and start_quote == 1 and end_quote == 0:\n",
    "            mid_quote = 1\n",
    "            quoted_words += text[char]\n",
    "\n",
    "        if text[char] == \"\\\"\" and start_quote == 1 and mid_quote == 1:\n",
    "            quoted_words = clean_incl_stopwords(quoted_words)\n",
    "            quote_list.append(quoted_words)\n",
    "            quote_word_len_list.append(count_words(quoted_words))\n",
    "\n",
    "            if count_words(quoted_words) > 0 and count_words(quoted_words) <= 3:\n",
    "                short_quotes.append(count_words(quoted_words))\n",
    "\n",
    "            if count_words(quoted_words) > 3:\n",
    "                long_quotes.append(count_words(quoted_words))\n",
    "            \n",
    "            mid_quote = 0\n",
    "            end_quote = 1\n",
    "\n",
    "    dict['quote_word_count'].append(sum(quote_word_len_list))\n",
    "    dict['quote_count'].append(len(quote_word_len_list))\n",
    "    dict['count_short'].append(len(short_quotes))\n",
    "    dict['count_long'].append(len(long_quotes))\n",
    "\n",
    "    if len(short_quotes) > 0:\n",
    "        dict['avg_len_short'].append(round(sum(short_quotes) / len(short_quotes),1))\n",
    "    else:\n",
    "        dict['avg_len_short'].append(0)\n",
    "\n",
    "    if len(long_quotes) > 0:\n",
    "        dict['avg_len_long'].append(round(sum(long_quotes) / len(long_quotes), 1))\n",
    "    else:\n",
    "        dict['avg_len_long'].append(0)\n",
    "\n",
    "    return pd.DataFrame(dict)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd = os.getcwd()\n",
    "file_list = os.listdir()\n",
    "columnist = wd.split('\\\\')[len(wd.split('\\\\'))-1:][0]\n",
    "\n",
    "# file names are in YYYYMMDD.txt format\n",
    "format = '%Y%m%d'\n",
    "\n",
    "dict = {\n",
    "\t\"columnist\":[],\n",
    "\t\"file_date\":[],\n",
    "\t\"story_punctuation\":[],\n",
    "\t\"word_count\":[],\n",
    "\t\"distinct_word_count\":[],\n",
    "\t'quote_word_count':[],\n",
    "\t\"sentence_count\":[],\n",
    "\t'quote_count':[],\n",
    "\t'count_short_quote':[],\n",
    "\t'count_long_quote':[],\n",
    "\t'avg_len_short_quote':[],\n",
    "\t'avg_len_long_quote':[]\n",
    "}\n",
    "\n",
    "dict_vocab = {\n",
    "\t\"columnist\":[],\n",
    "\t\"distinct_words\":[],\n",
    "\t\"distinct_words_excl_stop\":[],\n",
    "\t\"all_nonstop_words\":[]\n",
    "}\n",
    "\n",
    "vocab_w_stop = []\n",
    "vocab_no_stop = []\n",
    "all_words = []\n",
    "\n",
    "# iterate through all files\n",
    "for i in range(len(file_list)):\n",
    "\tfile = file_list[i]\n",
    "\tymd = file[:8]\n",
    "\n",
    "\t# Check whether file is in text format or not\n",
    "\tif file.endswith(\".txt\"):\n",
    "\t\tfile_path = f\"{wd}\\{file}\"\n",
    "\t\tfile_date = dt.datetime.strptime(ymd, format)\n",
    "\t\ts = read_text_file(file_path)\n",
    "\t\tcleaned_string = clean_string(s)\n",
    "\t\tclean_with_stopwords = clean_incl_stopwords(s)\n",
    "\t\tp = print_punctuation(s)\n",
    "\t\tvocab_no_stop = build_vocab(vocab_no_stop, cleaned_string)\n",
    "\t\tvocab_w_stop = build_vocab(vocab_w_stop, clean_with_stopwords)\n",
    "\t\tall_words = build_vocab(all_words, cleaned_string, distinct=False)\n",
    "\n",
    "\t\tdx = stats_quoted_words(s)\n",
    "\n",
    "\t\tdict[\"columnist\"].append(columnist)\n",
    "\t\tdict[\"file_date\"].append(file_date)\n",
    "\t\tdict[\"story_punctuation\"].append(p)\n",
    "\t\tdict[\"word_count\"].append(count_words(s))\n",
    "\t\tdict[\"distinct_word_count\"].append(count_distinct_words(s))\n",
    "\t\tdict[\"sentence_count\"].append(count_sentences(s))\n",
    "\t\tdict[\"quote_word_count\"].append(dx.quote_word_count[0])\n",
    "\t\tdict[\"quote_count\"].append(dx.quote_count[0])\n",
    "\t\tdict[\"count_short_quote\"].append(dx.count_short[0])\n",
    "\t\tdict[\"count_long_quote\"].append(dx.count_long[0])\n",
    "\t\tdict['avg_len_short_quote'].append(dx.avg_len_short[0])\n",
    "\t\tdict[\"avg_len_long_quote\"].append(dx.avg_len_long[0])\n",
    "\n",
    "dict_vocab[\"columnist\"].append(columnist)\n",
    "dict_vocab[\"distinct_words\"].append(vocab_w_stop)\n",
    "dict_vocab[\"distinct_words_excl_stop\"].append(vocab_no_stop)\n",
    "dict_vocab[\"all_nonstop_words\"].append(all_words)\n",
    "\n",
    "df = pd.DataFrame(dict)\n",
    "df_vocab = pd.DataFrame(dict_vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>biden</th>\n",
       "      <td>232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trump</th>\n",
       "      <td>205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>american</th>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>president</th>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>democrat</th>\n",
       "      <td>127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>america</th>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>white</th>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>person</th>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>state</th>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>would</th>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           freq\n",
       "word           \n",
       "biden       232\n",
       "trump       205\n",
       "american    155\n",
       "president   137\n",
       "democrat    127\n",
       "america     102\n",
       "white        84\n",
       "person       81\n",
       "state        80\n",
       "would        79"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hl = df_vocab.all_nonstop_words.to_list()[0]\n",
    "v = vocab_freq(hl, _singularize=True, top_n=10)\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ",.\",\".,-,.,.,-.,,',-.,-.,,-..,.,,.,,\".\",.\".\",,-.,-\n",
      "..,?,,.!-,,?,..,.,.,\",\"..,,.,..:\",,.\"?..,,.\"\":\":..\n",
      "..\";!\n",
      "\n",
      ",:\",,.,..\"!..,,,.:\",,.\".,.:\"(),.\".:\",,--?\"?:\",,,?\"\n",
      ",.,.,.\"\".,.\",\"..-\".\".,-...,..,-:\"...-.\"!,-\"\"..\n",
      "\n",
      ";,..!,./.,,.%..:,,,,,,;.,,,..,....!.,....,,.,..,,,\n",
      ".-..,,,.....,,.\n",
      "\n",
      "-.,,....,-.,,-.,.-.-.\",\"-?.-\"\".-\"\",-..\".\"-,\"\".,:\"?\n",
      "\":\".\",.?-?:.\"\"..,.,.,,,,!.-?:--,\".\",,.,.-,-,.,.().\n",
      "..,.\n",
      "\n",
      ".,??..,:,,,-?:\",,.\"??,?,.\"\".\"\".,,.,,.,.,,-..:,,.(%\n",
      ")-..,,.,-,(),.(,.)..(%),,,:(%).\"\".,.:,,--+.,-.\"\".\n",
      "\n",
      "...,.,,.\"\",,.,,,,.,.,.,-,.,.;....,..,.\"\".\"\",..,.,,\n",
      ",.,-.,\".\",,.,.,,..,.-,-,.:,,?,,\"\"\"\"?.,,?\n",
      "\n",
      ".,/...,...,.-.,-.??.,.,.?,,.:.......,..?..:\",,.\"--\n",
      ",..-,..\",\",.\"\".,:??,--?..\n",
      "\n",
      ".,,..,.......\",\",,..,..,..\"\"..\",\",,,,...,....,-,,-\n",
      "\".\"\".,.\":.\",\"-,,.,,\".\"-,.:,.-,\",\",-,.\n",
      "\n",
      "--....,.:,,,,,,,.,.,,\"\".,.,,.(--),,,,,,,,,,....,.-\n",
      ",?!,.?\"!\"!-\"\".,.,\"\"\"-.\",,.,.,.,\".\":,,.\",\",..\",\"\".\"\n",
      ".!\n",
      "\n",
      ":,\"\"\"\".--,,,.--,.\"\".,,\",,,.\":\".\",..,.\"\";-.,.?.,--,\n",
      ".,-.\"\",\".\":\"..\",,.,-,,,,\"\",!-.-,,,,\"\".-,-\".\",-.\n",
      "\n",
      ".,-.,.,,.\".\"\"-.\",.,,.,,.'.,.,,..,.,%,,.,-.,.,-..,.\n",
      ":\".\",.--?..-,...,\"\".,....,.,\"!\",-:\",.,,..?,,,,?.\"\n",
      "\n",
      "\",,.\"-.,:\",.\",,\"?!,,,.,.\"....,,,.....,,,$.%..,.,.,\n",
      ",.,,$.,,.,,.-..,,,..,,-.,,-,.,\".\"\n",
      "\n",
      "-:?.,,.\",.\",.-..,\".\"-\",\"..:\".,...,-,.\":\"---...,.\"-\n",
      ",,.\".\".,:\".\",:\",:..\",,.?,,.\"\"..?:,\".\".,-..,,,,!\n",
      "\n",
      ",/,.::\",..\"-,,.,,,..,,..\"?\"\"\"\"\"?,-:\",,\"\".\":-...,,\"\n",
      "\"\"\"?!\"?\"\"\",....:---,,-,.!,,,,,,.,.,.-..\n",
      "\n",
      "\"\",,,-.,,.\"\"-.:\".\",,,.!,:\"-!!\",:.,...,,.,-\"\"-,,.\"\"\n",
      "..,...-.,,--,-.,:\"!\",,..,.\"\".\n",
      "\n",
      "-,/,.\"..!\"!,,,.\",!\"-\"\".\"\",,...,-.,,-.,''.-.,.-.,,.\n",
      ".-\"\"?:.:\"-!\"\"\",,,.\"\".,\"\"-..,....!%,,..--...,,.%.?.\n",
      ",?.\n",
      "\n",
      ".,,,.,.:\",.,:,.-,,.,.?\"!--:\"...\"!,.,,,-.,\"\"-..,.,.\n",
      "\".\"-,,,:\"().\"!?\"..\"..,\".\",\".\"!;\"\"\".\"!:.\"\".,\"\"\"\".,.\n",
      ".,,,,,..\n",
      "\n",
      ":\".\":\"!\"!.\"\"',,,,..()..,.,,....()\"\"\".\"()\"\".;.()-..\n",
      ".;.(),-.,\".\"():\".\"()',,,,.(,)\"\",.(,)/.(,)!(,)-,,,.\n",
      "..-,,,,----.,,\"\"\",\".:-.--.,,,,,!\n",
      "\n",
      "\"----\".\",,.,.\":,-,.,-,,.!...,.,.,.,-.,,.,-..,....;\n",
      ".:\",,,.\".,\".\"()-,-,.,,,.(,)\",\"%...-,%.,%\"\".\",\".\"\",\n",
      ",;,,....\n",
      "\n",
      "-,,,..,.,.,.--..,.?,.-.(.)...-..:.:,,.!.!-.,..\"\"..\n",
      "...,..,...\".\",.\n",
      "\n",
      ":\",,().,...()-.,,.\"(\",\".\"\".)\",\",.-.\"\",,,,\"...\",,!-\n",
      "\"\"-,.,.\"-\",\"..,\"?:\",??--,.\",:\".\"?--./.-/.,,/./.\"?\"\n",
      "\"\"-?..\".\"\n",
      "\n",
      "\",\".\"','..\"'.\"\"?,-.-,.-\"\"--.,.\"\"-:...-..-..,..-.\"\"\n",
      "-:-......-,./.,\".\"..-,,...\"\"...,..\".\"-\"\"-,.\"!\"\n",
      "\n",
      "&\"\"-..:\"\"\"-.\",..,....,.,,.$,.-.?-/.,.,-.,,,-....!.\n",
      ",!-..,,,\"!\"\n",
      "\n",
      ".-,,\"\".-,,,..-..\"\"\"\",\".\"\",\"\"\"!-,,\"\"--\"\"\"!\",,\"\".\"\"\"\n",
      ",\"\"\".\"\",\"\"\"\"-.\".\"\"\",.-,!,/.\",\".,.-..\"\".,,-.\n",
      "\n",
      "...-,.-\",\",.\"\"\"\".\"\".\"\"\",,.\"\"\",.\".\"-,,,-,(%),(%).,-\n",
      "\".\",,-..,%.--.,.,,.,,,%,.(%)\".\".,-,(),.,,\".\":\"\".,.\n",
      ",,.,,\"\".\n",
      "\n",
      "...,\",\".\"\".,.,.,-..-,,,...,:),,)..\"\".,.\".\",,\"\".,,,\n",
      "??.:-:\";,..,;,.\"\n",
      "\n",
      ".-,.,,.,.....,-,,.-.\"\"().-,\"\".-.,.,....,.,,-,,,,.-\n",
      ",+,.-..,.,,......,,\"\"....,.,.\n",
      "\n",
      ",-,,,:\"..\":\".\"\"\"\"\"\"\"\".\",-.,\",\".,,..,,:\".\",\"\"\"\".-.,\n",
      ",.,.,:\".,,;.\",\".\".,.-,,,-,,-,-.\"\",-.-,-,-,\"\"-,,,,.\n",
      "\",\"\"\"-,.\n",
      "\n",
      "!.....,.\"\"/-,,,.,-..-,-.,,.,,,,.(.)...,.+-.,...$,,\n",
      "$,.-.,,-.\"\"..:,,,,,,,,,,,,,(,,),,-,!:,,.,\"\"\".\".,,,\n",
      ",-...!\n",
      "\n",
      "\",\"-,:\"',.\"\"\"....\".\".,,.\"\"\"\"-.,..,\".\",,,\"\".\",,\",,\"\n",
      "\".--,-..\".\",..\",\"\".\"-..,,-.,\".\",,!\",\"\n",
      "\n",
      "..,,().!\"\".,,:\",,.',,..\".,,.,\"\".,-,:\",,,,,,,..,,.\"\n",
      ".,,,\"\"\".\",,.,-.,.\"\"??\".\",,,,,.\n",
      "\n",
      ",,.,,.,,..,,,,,,,\"-!\".,,,.,...,.,,..,,.,,..,',.,..\n",
      ".,.,-.,.,,.,-..,.,,..,--\"\".--.\n",
      "\n",
      ":\"\",\"\",\"\",\"\",\".\":.--.%%.-:..:,.:%%.,!--:..-.-,,.-.\n",
      ".,,.!,.\"\"\".\":\"'.\"(.).,--,,,-.\"\"\".\"...\"\"..(-).\",!\"?\n",
      "?,,\".\"!\n",
      "\n",
      ",,,./.--.,,.,-.,,.,,,,.-...,.,.(.)-,,-,,-.,,..,,.,\n",
      ".,...,-.,,,,.;.,\"\",,.,,,-,.,,,,.,.,,.\n",
      "\n",
      ".,-,......,:\".\",,,,,....,..,--..,\"--,\".,,.,,....,;\n",
      ".:,?,()...,-...,.\n",
      "\n",
      ",,,.\".\"\"\"\"\"-.:\"',''.\",!'..,,,-...\"\"%%,%%..,......,\n",
      ",,...?,,-.,.\"\".!,.,-./-?..!.,!,\".\"\n",
      "\n",
      ".,.,.\"\"-\"-.\",-,,,.\"\".\"\"\"\".\"\"?,-.-.,./,..-..,.,,..,\n",
      ".,,.--.,\".\",,.---,,../,-.,,.,.\n",
      "\n",
      ".\"\",..,...,..,....,,.?,-.-?-?-.\"\"...,\",\".,-.$+....\n",
      "\"\"\"\",--,-!?.,.?,.,-,.-,,,,...-...,.\n",
      "\n",
      "\"\"...\"\"\".\"\"\",,\".\",.,\"\"\".\",..'\"\".,.,\",\"\",\"\"\"...\"\"\",\n",
      ",-.\",\"\"\"\".\"-\",\".\"-..,\":,\".,,..\"\"..\".,,,-,,,\".\",,.\"\n",
      "\n",
      "\n",
      "....,..,..,.,,\".\".\",\"'.\"..\"\"-..,,,\".\".,,\"'.,,.,..$\n",
      ".,..',..,....:??,,??,!\n",
      "\n",
      ",:\".\"\",.,.,,.,.\".,,.-,..,.,.-.:,.\"\",.,\",\".\",\"\".\".-\n",
      ".,,-,.\".\":\"-.\".-..,:\"\".-.-.\".\",,.\n",
      "\n",
      ",...-.,.,,\"\"\".\"...,.,\"\"\".\"-.,,,\"\"..--,,..,..,,.,.,\n",
      "\".\"\"\"\".\"-....-.-,,,-.\n",
      "\n",
      ",...-.,.,,\"\"\".\"...,.,\"\"\".\"-.,,,\"\"..--,,..,..,,.,.,\n",
      "\".\"\"\"\".\"-....-.-,,,-.\n",
      "\n",
      "\"\"--.,-,,.!,.-,,.\":,,\"\":,,\"-.-.,.,,-!,,,,,,.,-,,-.\n",
      "..:?,,'-.,\"-\"...?,,,,,.\n",
      "\n",
      ".-.,,....,..,.,-.,./.?,.,-.,,.-...,-.(---)?,..,,,,\n",
      "-.!,-,%-,--,-,,.,\".\",.....,..,,--.\n",
      "\n",
      "..\",\".,.,,.,/,.-.,..\".\",-.,/,.,,.-..,..\"\"\"\"\"\".,\"\"!\n",
      "(!).,,.,.--,.,-,..?,,.\"\"?,,,.\"?\",.,?\n",
      "\n",
      ",,...,,,--...-.-.$.-,..,,,.,,..%.%,,%.,,.,..-,.--.\n",
      "\"\".\".\"..-?.,,.\n",
      "\n",
      "\"-,.,.\":\",..,..,..-.\".()--.,..-,,.:\"..?.\".\",\"..().\n",
      "\"\"\".\",.\"\"$..,,.,,-..---..:\"-...,-,!\"-,,,,,,,-.,...\n",
      ",,.\n",
      "\n",
      "\".\"\"\"\"\"\".\",\"\"\"\".\"\"..!,--..,,,,,,.,,.-,,,,,..-..,,.\n",
      "..,,-,...$.\"\".,....---..,-?\"!\"\n",
      "\n",
      ",..-,,,.,..---..,-..,.-,,..,,..\"\",-.,.,,,.,.,.\"\".,\n",
      "'.,.\"\"......,.,.,.,,?\n",
      "\n",
      "....,(,,,),.,.,,,,.,.,,,.(:\"\"\"!\"),-.,:\",-.\"...,..,\n",
      ".,-.,\"--\".\"\".,\"\"...,\"\",,.,,.,,\",\",,.\"\".\n",
      "\n",
      ",,\"\".-....,,:\",...,():.\":\".,,-,.-,-...,.\"-\"\",..,,.\n",
      "...,\".\"-..(\"\").\"\".%.%'-,.%,(),..-,..-.,,,,.\n",
      "\n",
      ",,.-:\",.-,--.\":\",.\"..,,,,,\"\".:\",,.,..\",\",\"\",\"\",\".,\n",
      ",()$,().'.,.,..,.,,\".,.\",,\"\"\".\".\",\"..\n",
      "\n",
      "\"\",...,.---,,...\"\"\",\"\"\"\",\".\",\",,..\"\",,,.:\",.\",,\"\"\"\n",
      "?\"-\".\":\".\"..,,..,,,,,,.,,.\":.%:.\"'.\"\":\",\"-,.\n",
      "\n",
      "..(,,,,,).-.:\",,-.\"\"\",.\"\",\"\"!...-...\"\"-..,-.,.,,,.\n",
      "\",\",,.,.,.'-.,/,.\"\".,-\".\".\"\"..,,\"\"\"!\",-,,.,?\n",
      "\n",
      ",-.\":..\"..,.,..,,,..,-,.,-.-.-...,.,.,..-,....,,,,\n",
      ".,..\n",
      "\n",
      "-,.\"\"\".\"....\"\".,,-.,,-.\"\"(!).,,,,,,,,,-.,,..,..,\"&\n",
      ",\".,-.,,.,!,',..,:.\n",
      "\n",
      "%..\"\".-\",\"-..-.\"\"...,\",\".-.,---.-..,.\".,\"..-...,,.\n",
      ".\"\".-,,...:,,,,,,-;,-.,-.,.,.\".\"(.)!.\"\"!\n",
      "\n",
      "-:\".\",.-..,.-,.,\"\",,..,.,..,.-,?..,,\"--\",,-.,-,.,/\n",
      "-.,?,.,,-,-,--,,-.-..,\"\"\",,.\"\n",
      "\n",
      ".,,.-.,..,,..-?\"!\",,,.\",\".\",..\"(.!)..,-,...,\".-.\",\n",
      ",,-,.,,.-.,..,,,?.-.,-',..,?\n",
      "\n",
      ".-.-.,:\"!\",..,.,,\"',,,.\",..,\"\".,,,-,-.\"\"..,.\".\".-.\n",
      ",.,\".\"!,..,.!?!.!??,,...,,.,.!-...,---\".\"\n",
      "\n",
      ",\"\"\",,.\".,,.?,,,,;.\"-\",-,,.,,!?....,-.?--,.,,.--,-\n",
      ".?!,,,,-,..,.,-.-.-,.!--,.,,,-,-.,,,,.,\"%\".\"\"..,\",\n",
      ",,,.\"\n",
      "\n",
      ",\",.\"!..:\"-..\",,,.:\":?-?\",\",,,,-..,,.\",\"\",-\"-\",?..\n",
      ",.,\"\"-\"?\"\"\"\"\"\"\"?,,.,,.,,.,,-..\"\"\"\",,.,,.,,,.,,.!.\n",
      "\n",
      "\"?\",.,.,-.--..-\",\",\",\".-:\"..\",!,:\"..,,....\":\"?\".--\n",
      ".,,--.:\",.\":\"....\".,,,,.,,,-.\n",
      "\n",
      ",.,.-,,,\".\",,.,.,,%,\"\".%.\"-,.\".\";.\".\".\",\"\".,-?:\"..\n",
      ".\"...-,\"\"-.,,.,.\n",
      "\n",
      ",:,-,.,.,,,-.\"-\",.,.-.\"\".!,.,--,-,,--,.-,,..-..\"\".\n",
      ",-.-,,,.,.--..,\"\".-,.-.,-\"\"-,,-.,-,.,,.,:.,,.\n",
      "\n",
      ".,.,,.-(!),..-.,,.:\",-,.\",..\"?\"(-:,,,,,-,!)\"-\",--,\n",
      ",\"\"\"-.\",-.\"-\".,\"-\"--.,\"-\".,-.-.,..,,.,\"\"\".\"..,,.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(df)):\n",
    "    print(stack_string(df['story_punctuation'][i], 50))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7e4608ffab33544aecd2f97721d910c9b0c38b0c6fa6ab7667a80c134f821554"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
