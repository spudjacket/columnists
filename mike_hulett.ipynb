{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import string\n",
    "import datetime as dt\n",
    "import pathlib\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import string\n",
    "import re\n",
    "from pattern.text.en import singularize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_document_list(path):\n",
    "    wd = os.getcwd()\n",
    "    file_list = os.listdir()\n",
    "    return(file_list)\n",
    "\n",
    "def read_text_file(file_path):\n",
    "\twith open(file_path, 'r', encoding='utf-8' ) as f:\n",
    "\t# with open(file_path, 'r', encoding='cp1252' ) as f:\n",
    "\t\treturn f.read()\n",
    "\t\t\n",
    "def print_punctuation(input_string=None):\n",
    "\tpunct_str = \"\"\n",
    "\tfor i in range(len(input_string)):\n",
    "\t\tchar = input_string[i] \n",
    "\t\tchar = char.replace('”', \"\\\"\")\n",
    "\t\tchar = char.replace('“', \"\\\"\")\n",
    "\t\tchar = char.replace(\"\\n\", \" \")\n",
    "\t\tif char in string.punctuation:\n",
    "\t\t\tpunct_str = punct_str + char\n",
    "\treturn punct_str\n",
    "\n",
    "def text_lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "def remove_numbers(text):\n",
    "    result = re.sub(r'\\d+', '', text)\n",
    "    return result\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(translator)    \n",
    "\n",
    "def remove_whitespace(text):\n",
    "    return  \" \".join(text.split())\n",
    "\n",
    "def count_sentences(text):\n",
    "\treturn len(sent_tokenize(text))\n",
    "\n",
    "def count_words(text):\n",
    "    return len(word_tokenize(text))\n",
    "    # word_count = [n+1 for word in word_tokens]\n",
    "\n",
    "def count_distinct_words(text):\n",
    "    return len(set(word_tokenize(text)))\n",
    "\n",
    "def build_vocab(existing_list, new_text, distinct=True):\n",
    "    new_words = word_tokenize(new_text)\n",
    "    existing_list.extend(new_words)\n",
    "    if distinct==True:\n",
    "        return list(set(existing_list))\n",
    "    else:\n",
    "        return(list(existing_list))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_text = [word for word in word_tokens if word not in stop_words]\n",
    "    return filtered_text\n",
    "\n",
    "def clean_string(text):\n",
    "    text = text.replace(\"’s\", \"\")\n",
    "    text = text.replace(\"'s\", \"\")\n",
    "    text = text.replace('”', \"\\\"\")\n",
    "    text = text.replace('“', \"\\\"\")\n",
    "    text = text.replace('’', \"\\'\")\n",
    "    # adding this to split hyptenated words into separate words\n",
    "    text = text.replace('-', \" \")\n",
    "    text = text_lowercase(text)\n",
    "    text = remove_numbers(text)\n",
    "    text = remove_punctuation(text)\n",
    "    text = remove_whitespace(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = ' '.join([str(elem) for elem in text])\n",
    "    return text\t\n",
    "\n",
    "def clean_incl_stopwords(text):\n",
    "    text = text.replace('”', \"\\\"\")\n",
    "    text = text.replace('“', \"\\\"\")\n",
    "    text = text.replace('’', \"\\'\")\n",
    "    # adding this to split hyptenated words into separate words\n",
    "    text = text.replace('-', \" \")\n",
    "    text = text_lowercase(text)\n",
    "    text = remove_numbers(text)\n",
    "    text = remove_punctuation(text)\n",
    "    text = remove_whitespace(text)\n",
    "    # text = remove_stopwords(text)\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_text = [word for word in word_tokens]\n",
    "    text = ' '.join([str(elem) for elem in filtered_text])\n",
    "    return text\t    \n",
    "\n",
    "def vocab_freq(word_list,\n",
    "               _singularize=False,\n",
    "               top_n=None):\n",
    "\n",
    "    _dict = {\n",
    "        'word':[],\n",
    "        'freq':[]\n",
    "    }\n",
    "\n",
    "    if _singularize == True:\n",
    "        singles = [singularize(plural) for plural in word_list]\n",
    "        distinct = list(set(singles))\n",
    "        for d in range(len(distinct)):\n",
    "            word = distinct[d]\n",
    "            freq = singles.count(word)\n",
    "            _dict['word'].append(word)\n",
    "            _dict['freq'].append(freq)\n",
    "    else:\n",
    "        distinct = list(set(word_list))\n",
    "        for d in range(len(distinct)):\n",
    "            word = distinct[d]\n",
    "            freq = word_list.count(word)\n",
    "            _dict['word'].append(word)\n",
    "            _dict['freq'].append(freq)\n",
    "\n",
    "    df = pd.DataFrame(_dict)\n",
    "    df.sort_values(by='freq', ascending=False, inplace=True)\n",
    "    df.set_index('word', inplace=True)\n",
    "    \n",
    "    if top_n:\n",
    "        return df.head(top_n)\n",
    "    else:\n",
    "        return df\n",
    "\n",
    "def stack_string(text, max_length=25):\n",
    "    stacked = ''\n",
    "    n = 0\n",
    "    for i in range(len(text)):\n",
    "        stacked = stacked + text[i]\n",
    "        n+=1\n",
    "        if n == max_length:\n",
    "            stacked = stacked + '\\n'\n",
    "            n = 0\n",
    "    return stacked + '\\n'    \n",
    "\n",
    "def count_quoted_words(text):\n",
    "    text = text.replace('”', \"\\\"\")\n",
    "    text = text.replace('“', \"\\\"\")\n",
    "    quoted_words = ''\n",
    "    start_quote = 0\n",
    "    mid_quote = 0\n",
    "    end_quote = 0\n",
    "    for char in range(len(text)):\n",
    "        if start_quote == 1 and end_quote == 1:\n",
    "            start_quote = 0\n",
    "            end_quote = 0\n",
    "\n",
    "        if text[char] == \"\\\"\" and start_quote == 0 and end_quote == 0:\n",
    "            start_quote = 1\n",
    "\n",
    "        if text[char] != \"\\\"\" and start_quote == 1 and end_quote == 0:\n",
    "            mid_quote = 1\n",
    "            quoted_words += text[char]\n",
    "\n",
    "        if text[char] == \"\\\"\" and start_quote == 1 and mid_quote == 1:\n",
    "            quoted_words += \" \"\n",
    "            mid_quote = 0\n",
    "            end_quote = 1\n",
    "\n",
    "    quoted_words = clean_incl_stopwords(quoted_words)\n",
    "    word_count = count_words(quoted_words)\n",
    "\n",
    "    return word_count\n",
    "\n",
    "def stats_quoted_words(text):\n",
    "    text = text.replace('”', \"\\\"\")\n",
    "    text = text.replace('“', \"\\\"\")\n",
    "    quoted_words = ''\n",
    "    start_quote = 0\n",
    "    mid_quote = 0\n",
    "    end_quote = 0\n",
    "    short_quotes = []\n",
    "    long_quotes = []\n",
    "    quote_list = []\n",
    "    quote_word_len_list = []\n",
    "    \n",
    "    dict = {\n",
    "        'quote_word_count':[],\n",
    "        'quote_count':[],\n",
    "        'count_short':[],\n",
    "        'count_long':[],\n",
    "        'avg_len_short':[],\n",
    "        'avg_len_long':[]\n",
    "    }\n",
    "\n",
    "    for char in range(len(text)):\n",
    "        if start_quote == 1 and end_quote == 1:\n",
    "            quoted_words = ''\n",
    "            start_quote = 0\n",
    "            end_quote = 0\n",
    "\n",
    "        if text[char] == \"\\\"\" and start_quote == 0 and end_quote == 0:\n",
    "            start_quote = 1\n",
    "\n",
    "        if text[char] != \"\\\"\" and start_quote == 1 and end_quote == 0:\n",
    "            mid_quote = 1\n",
    "            quoted_words += text[char]\n",
    "\n",
    "        if text[char] == \"\\\"\" and start_quote == 1 and mid_quote == 1:\n",
    "            quoted_words = clean_incl_stopwords(quoted_words)\n",
    "            quote_list.append(quoted_words)\n",
    "            quote_word_len_list.append(count_words(quoted_words))\n",
    "\n",
    "            if count_words(quoted_words) > 0 and count_words(quoted_words) <= 3:\n",
    "                short_quotes.append(count_words(quoted_words))\n",
    "\n",
    "            if count_words(quoted_words) > 3:\n",
    "                long_quotes.append(count_words(quoted_words))\n",
    "            \n",
    "            mid_quote = 0\n",
    "            end_quote = 1\n",
    "\n",
    "    dict['quote_word_count'].append(sum(quote_word_len_list))\n",
    "    dict['quote_count'].append(len(quote_word_len_list))\n",
    "    dict['count_short'].append(len(short_quotes))\n",
    "    dict['count_long'].append(len(long_quotes))\n",
    "\n",
    "    if len(short_quotes) > 0:\n",
    "        dict['avg_len_short'].append(round(sum(short_quotes) / len(short_quotes),1))\n",
    "    else:\n",
    "        dict['avg_len_short'].append(0)\n",
    "\n",
    "    if len(long_quotes) > 0:\n",
    "        dict['avg_len_long'].append(round(sum(long_quotes) / len(long_quotes), 1))\n",
    "    else:\n",
    "        dict['avg_len_long'].append(0)\n",
    "\n",
    "    return pd.DataFrame(dict)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting directory path of top of repository\n",
    "dir_top = os.getcwd()\n",
    "columnist = 'Mike Hulett'\n",
    "\n",
    "# file names are in YYYYMMDD.txt format\n",
    "format = '%Y%m%d'\n",
    "columnist_files = os.listdir(os.chdir(dir_top + './' + columnist))\n",
    "\n",
    "dict = {\n",
    "\t\"columnist\":[],\n",
    "\t\"file_date\":[],\n",
    "\t\"story_punctuation\":[],\n",
    "\t\"word_count\":[],\n",
    "\t\"distinct_word_count\":[],\n",
    "\t'quote_word_count':[],\n",
    "\t\"sentence_count\":[],\n",
    "\t'quote_count':[],\n",
    "\t'count_short_quote':[],\n",
    "\t'count_long_quote':[],\n",
    "\t'avg_len_short_quote':[],\n",
    "\t'avg_len_long_quote':[],\n",
    "\t'all_words':[]\n",
    "}\n",
    "\n",
    "dict_vocab = {\n",
    "\t\"columnist\":[],\n",
    "\t\"distinct_words\":[],\n",
    "\t\"distinct_words_excl_stop\":[],\n",
    "\t\"all_nonstop_words\":[]\n",
    "}\n",
    "\n",
    "dict_sentences = {\n",
    "\t\"columnist\":[],\n",
    "\t\"file_date\":[],\n",
    "\t\"text\":[]\n",
    "}\n",
    "\n",
    "vocab_w_stop = []\n",
    "vocab_no_stop = []\n",
    "all_words = []\n",
    "columnist_sentences = []\n",
    "list_bigrams = []\n",
    "list_trigrams = []\n",
    "\n",
    "# iterate through all files\n",
    "for i in range(len(columnist_files)):\n",
    "\tfile = columnist_files[i]\n",
    "\tymd = file[:8]\n",
    "\n",
    "\t# Check whether file is in text format or not\n",
    "\tif file.endswith(\".txt\"):\n",
    "\t\t# file_path = f\"{wd}\\{file}\"\n",
    "\t\tfile_date = dt.datetime.strptime(ymd, format)\n",
    "\t\ts = read_text_file(file)\n",
    "\t\tcleaned_string = clean_string(s)\n",
    "\t\tclean_with_stopwords = clean_incl_stopwords(s)\n",
    "\t\tp = print_punctuation(s)\n",
    "\t\tvocab_no_stop = build_vocab(vocab_no_stop, cleaned_string)\n",
    "\t\tvocab_w_stop = build_vocab(vocab_w_stop, clean_with_stopwords)\n",
    "\t\tall_words = build_vocab(all_words, cleaned_string, distinct=False)\n",
    "\n",
    "\t\twords = nltk.word_tokenize(cleaned_string)\n",
    "\t\tlist_bigrams.extend(list(nltk.bigrams(words)))\n",
    "\t\tlist_trigrams.extend(list(nltk.trigrams(words)))\n",
    "\t\t\n",
    "\t\tcolumnist_sentences = sent_tokenize(s)\n",
    "\t\tfor i in range(len(columnist_sentences)):\n",
    "\t\t\tdict_sentences[\"columnist\"].append(columnist)\n",
    "\t\t\tdict_sentences[\"file_date\"].append(file_date)\n",
    "\t\t\tdict_sentences[\"text\"].append(text_lowercase(columnist_sentences[i]))\n",
    "\n",
    "\t\tdx = stats_quoted_words(s)\n",
    "\n",
    "\t\tdict[\"columnist\"].append(columnist)\n",
    "\t\tdict[\"file_date\"].append(file_date)\n",
    "\t\tdict[\"story_punctuation\"].append(p)\n",
    "\t\tdict[\"word_count\"].append(count_words(s))\n",
    "\t\tdict[\"distinct_word_count\"].append(count_distinct_words(s))\n",
    "\t\tdict[\"sentence_count\"].append(count_sentences(s))\n",
    "\t\tdict[\"quote_word_count\"].append(dx.quote_word_count[0])\n",
    "\t\tdict[\"quote_count\"].append(dx.quote_count[0])\n",
    "\t\tdict[\"count_short_quote\"].append(dx.count_short[0])\n",
    "\t\tdict[\"count_long_quote\"].append(dx.count_long[0])\n",
    "\t\tdict['avg_len_short_quote'].append(dx.avg_len_short[0])\n",
    "\t\tdict[\"avg_len_long_quote\"].append(dx.avg_len_long[0])\n",
    "\t\tdict[\"all_words\"].append(cleaned_string)\n",
    "\n",
    "dict_vocab[\"columnist\"].append(columnist)\n",
    "dict_vocab[\"distinct_words\"].append(vocab_w_stop)\n",
    "dict_vocab[\"distinct_words_excl_stop\"].append(vocab_no_stop)\n",
    "dict_vocab[\"all_nonstop_words\"].append(all_words)\n",
    "\n",
    "df = pd.DataFrame(dict)\n",
    "df_vocab = pd.DataFrame(dict_vocab)\n",
    "df_sentences = pd.DataFrame(dict_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>columnist</th>\n",
       "      <th>file_date</th>\n",
       "      <th>story_punctuation</th>\n",
       "      <th>word_count</th>\n",
       "      <th>distinct_word_count</th>\n",
       "      <th>quote_word_count</th>\n",
       "      <th>sentence_count</th>\n",
       "      <th>quote_count</th>\n",
       "      <th>count_short_quote</th>\n",
       "      <th>count_long_quote</th>\n",
       "      <th>avg_len_short_quote</th>\n",
       "      <th>avg_len_long_quote</th>\n",
       "      <th>all_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mike Hulett</td>\n",
       "      <td>2020-04-11</td>\n",
       "      <td>,.\",\".,-,.,.,-.,,',-.,-.,,-..,.,,.,,\".\",.\".\",,...</td>\n",
       "      <td>631</td>\n",
       "      <td>374</td>\n",
       "      <td>71</td>\n",
       "      <td>28</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>16.2</td>\n",
       "      <td>wuhan virus dominates presidential election lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mike Hulett</td>\n",
       "      <td>2020-04-18</td>\n",
       "      <td>,:\",,.,..\"!..,,,.:\",,.\".,.:\"(),.\".:\",,--?\"?:\",...</td>\n",
       "      <td>607</td>\n",
       "      <td>321</td>\n",
       "      <td>176</td>\n",
       "      <td>29</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1.7</td>\n",
       "      <td>24.4</td>\n",
       "      <td>april th column jim shaw wrote last two weeks ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mike Hulett</td>\n",
       "      <td>2020-05-09</td>\n",
       "      <td>;,..!,./.,,.%..:,,,,,,;.,,,..,....!.,....,,.,....</td>\n",
       "      <td>585</td>\n",
       "      <td>314</td>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>dollar every forum editorial disagreed years w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mike Hulett</td>\n",
       "      <td>2020-05-23</td>\n",
       "      <td>-.,,....,-.,,-.,.-.-.\",\"-?.-\"\".-\"\",-..\".\"-,\"\"....</td>\n",
       "      <td>616</td>\n",
       "      <td>361</td>\n",
       "      <td>36</td>\n",
       "      <td>30</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1.2</td>\n",
       "      <td>7.5</td>\n",
       "      <td>declassified justice department documents conf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mike Hulett</td>\n",
       "      <td>2020-06-20</td>\n",
       "      <td>.,??..,:,,,-?:\",,.\"??,?,.\"\".\"\".,,.,,.,.,,-..:,...</td>\n",
       "      <td>591</td>\n",
       "      <td>303</td>\n",
       "      <td>19</td>\n",
       "      <td>31</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>moorhead mayor jonathan judd sensibly proposed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>Mike Hulett</td>\n",
       "      <td>2022-10-08</td>\n",
       "      <td>\"?\",.,.,-.--..-\",\",\",\".-:\"..\",!,:\"..,,....\":\"?...</td>\n",
       "      <td>587</td>\n",
       "      <td>337</td>\n",
       "      <td>181</td>\n",
       "      <td>20</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.6</td>\n",
       "      <td>opinion website featured bizarre headline qano...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>Mike Hulett</td>\n",
       "      <td>2022-10-22</td>\n",
       "      <td>,.,.-,,,\".\",,.,.,,%,\"\".%.\"-,.\".\";.\".\".\",\"\".,-?...</td>\n",
       "      <td>572</td>\n",
       "      <td>320</td>\n",
       "      <td>142</td>\n",
       "      <td>18</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>former speaker house newt gingrich highly resp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>Mike Hulett</td>\n",
       "      <td>2022-11-05</td>\n",
       "      <td>,:,-,.,.,,,-.\"-\",.,.-.\"\".!,.,--,-,,--,.-,,..-....</td>\n",
       "      <td>583</td>\n",
       "      <td>349</td>\n",
       "      <td>14</td>\n",
       "      <td>27</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>moderate democrat independent republican reade...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>Mike Hulett</td>\n",
       "      <td>2022-11-19</td>\n",
       "      <td>.,.,,.-(!),..-.,,.:\",-,.\",..\"?\"(-:,,,,,-,!)\"-\"...</td>\n",
       "      <td>589</td>\n",
       "      <td>336</td>\n",
       "      <td>85</td>\n",
       "      <td>21</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.1</td>\n",
       "      <td>focusing issues every opinion columnist expect...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>Mike Hulett</td>\n",
       "      <td>2022-12-03</td>\n",
       "      <td>.-?,!,,,...,,,...,..,,.....(,,\"\".!).,...!,.,.,...</td>\n",
       "      <td>567</td>\n",
       "      <td>327</td>\n",
       "      <td>3</td>\n",
       "      <td>36</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>global warming cultists allege many conservati...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>67 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      columnist  file_date                                  story_punctuation  \\\n",
       "0   Mike Hulett 2020-04-11  ,.\",\".,-,.,.,-.,,',-.,-.,,-..,.,,.,,\".\",.\".\",,...   \n",
       "1   Mike Hulett 2020-04-18  ,:\",,.,..\"!..,,,.:\",,.\".,.:\"(),.\".:\",,--?\"?:\",...   \n",
       "2   Mike Hulett 2020-05-09  ;,..!,./.,,.%..:,,,,,,;.,,,..,....!.,....,,.,....   \n",
       "3   Mike Hulett 2020-05-23  -.,,....,-.,,-.,.-.-.\",\"-?.-\"\".-\"\",-..\".\"-,\"\"....   \n",
       "4   Mike Hulett 2020-06-20  .,??..,:,,,-?:\",,.\"??,?,.\"\".\"\".,,.,,.,.,,-..:,...   \n",
       "..          ...        ...                                                ...   \n",
       "62  Mike Hulett 2022-10-08  \"?\",.,.,-.--..-\",\",\",\".-:\"..\",!,:\"..,,....\":\"?...   \n",
       "63  Mike Hulett 2022-10-22  ,.,.-,,,\".\",,.,.,,%,\"\".%.\"-,.\".\";.\".\".\",\"\".,-?...   \n",
       "64  Mike Hulett 2022-11-05  ,:,-,.,.,,,-.\"-\",.,.-.\"\".!,.,--,-,,--,.-,,..-....   \n",
       "65  Mike Hulett 2022-11-19  .,.,,.-(!),..-.,,.:\",-,.\",..\"?\"(-:,,,,,-,!)\"-\"...   \n",
       "66  Mike Hulett 2022-12-03  .-?,!,,,...,,,...,..,,.....(,,\"\".!).,...!,.,.,...   \n",
       "\n",
       "    word_count  distinct_word_count  quote_word_count  sentence_count  \\\n",
       "0          631                  374                71              28   \n",
       "1          607                  321               176              29   \n",
       "2          585                  314                 0              33   \n",
       "3          616                  361                36              30   \n",
       "4          591                  303                19              31   \n",
       "..         ...                  ...               ...             ...   \n",
       "62         587                  337               181              20   \n",
       "63         572                  320               142              18   \n",
       "64         583                  349                14              27   \n",
       "65         589                  336                85              21   \n",
       "66         567                  327                 3              36   \n",
       "\n",
       "    quote_count  count_short_quote  count_long_quote  avg_len_short_quote  \\\n",
       "0             7                  3                 4                  2.0   \n",
       "1            10                  3                 7                  1.7   \n",
       "2             0                  0                 0                  0.0   \n",
       "3             9                  5                 4                  1.2   \n",
       "4             5                  4                 1                  2.0   \n",
       "..          ...                ...               ...                  ...   \n",
       "62            8                  0                 8                  0.0   \n",
       "63            8                  3                 5                  3.0   \n",
       "64            5                  5                 0                  2.8   \n",
       "65           10                  2                 8                  2.0   \n",
       "66            2                  2                 0                  1.5   \n",
       "\n",
       "    avg_len_long_quote                                          all_words  \n",
       "0                 16.2  wuhan virus dominates presidential election lo...  \n",
       "1                 24.4  april th column jim shaw wrote last two weeks ...  \n",
       "2                  0.0  dollar every forum editorial disagreed years w...  \n",
       "3                  7.5  declassified justice department documents conf...  \n",
       "4                 11.0  moorhead mayor jonathan judd sensibly proposed...  \n",
       "..                 ...                                                ...  \n",
       "62                22.6  opinion website featured bizarre headline qano...  \n",
       "63                26.6  former speaker house newt gingrich highly resp...  \n",
       "64                 0.0  moderate democrat independent republican reade...  \n",
       "65                10.1  focusing issues every opinion columnist expect...  \n",
       "66                 0.0  global warming cultists allege many conservati...  \n",
       "\n",
       "[67 rows x 13 columns]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26.223880597014926\n"
     ]
    }
   ],
   "source": [
    "wc = df.groupby(['columnist'])['word_count'].mean().reset_index()\n",
    "lc = df.groupby(['columnist'])['avg_len_long_quote'].mean().reset_index()\n",
    "sq = df.groupby(['columnist'])['count_short_quote'].mean().reset_index()\n",
    "lq = df.groupby(['columnist'])['count_long_quote'].mean().reset_index()\n",
    "\n",
    "sq = df[df.count_long_quote > 0]\n",
    "sq = list(df.sentence_count.to_list())\n",
    "\n",
    "print(np.average(sq))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     columnist  word_count\n",
      "0  Mike Hulett  590.432836      columnist  avg_len_long_quote\n",
      "0  Mike Hulett           15.132836      columnist  count_short_quote\n",
      "0  Mike Hulett           4.761194      columnist  count_long_quote\n",
      "0  Mike Hulett          2.925373\n"
     ]
    }
   ],
   "source": [
    "print(wc, lc, sq, lq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct_words = df_vocab.distinct_words_excl_stop.iloc[0]\n",
    "all_words = df_vocab.all_nonstop_words.iloc[0]\n",
    "\n",
    "dict_words = {\n",
    "    'word':[],\n",
    "    'count':[]\n",
    "}\n",
    "\n",
    "for i in range(len(words)):\n",
    "    w = words[i]\n",
    "    dict_words['word'].append(w)\n",
    "    dict_words['count'].append(all_words.count(w))\n",
    "\n",
    "df_words = pd.DataFrame(dict_words)\n",
    "\n",
    "unique_bigrams = list(set(list_bigrams))\n",
    "unique_trigrams = list(set(list_trigrams))\n",
    "\n",
    "dict_bigrams = {\n",
    "    'bigram':[],\n",
    "    'count':[]\n",
    "}\n",
    "\n",
    "dict_trigrams = {\n",
    "    'trigram':[],\n",
    "    'count':[]\n",
    "}\n",
    "\n",
    "for i in range(len(unique_bigrams)):\n",
    "    b = unique_bigrams[i]\n",
    "    c = list_bigrams.count(b)\n",
    "    dict_bigrams['bigram'].append(b)\n",
    "    dict_bigrams['count'].append(c)\n",
    "\n",
    "for i in range(len(unique_trigrams)):\n",
    "    b = unique_trigrams[i]\n",
    "    c = list_trigrams.count(b)\n",
    "    dict_trigrams['trigram'].append(b)\n",
    "    dict_trigrams['count'].append(c)\n",
    "\n",
    "df_bigrams = pd.DataFrame(dict_bigrams)\n",
    "df_trigrams = pd.DataFrame(dict_trigrams)    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def count_docs_contain_term(string=None):\n",
    "#     count = 0\n",
    "#     for i in range(len(columnist_files)):\n",
    "#         file = columnist_files[i]\n",
    "#         s = read_text_file(file)\n",
    "#         cleaned_string = clean_string(s)\n",
    "#         if cleaned_string.count(term) > 0:\n",
    "#             count += 1\n",
    "#     return count\n",
    "\n",
    "# def inverse_doc_freq(term):\n",
    "#     for i in range(len(columnist_files)):\n",
    "#         file = columnist_files[i]\n",
    "#         s = read_text_file(file)\n",
    "#         cleaned_string = clean_string(s)\n",
    "#         if cleaned_string.count(term) > 0:\n",
    "#             document_contains_term += 1\n",
    "#     return np.log(len(columnist_files) / document_contains_term)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trigram</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16935</th>\n",
       "      <td>(black, lives, matter)</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13221</th>\n",
       "      <td>(president, joe, biden)</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13297</th>\n",
       "      <td>(president, donald, trump)</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10130</th>\n",
       "      <td>(former, president, donald)</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5147</th>\n",
       "      <td>(border, patrol, agents)</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20037</th>\n",
       "      <td>(speaker, nancy, pelosi)</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15571</th>\n",
       "      <td>(critical, race, theory)</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5692</th>\n",
       "      <td>(fargo, moorhead, west)</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5507</th>\n",
       "      <td>(moorhead, west, fargo)</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12181</th>\n",
       "      <td>(vice, president, kamala)</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           trigram  count\n",
       "16935       (black, lives, matter)     21\n",
       "13221      (president, joe, biden)     13\n",
       "13297   (president, donald, trump)      9\n",
       "10130  (former, president, donald)      9\n",
       "5147      (border, patrol, agents)      7\n",
       "20037     (speaker, nancy, pelosi)      7\n",
       "15571     (critical, race, theory)      7\n",
       "5692       (fargo, moorhead, west)      7\n",
       "5507       (moorhead, west, fargo)      7\n",
       "12181    (vice, president, kamala)      6"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_trigrams.sort_values(by='count', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>columnist</th>\n",
       "      <th>file_date</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>Mike Hulett</td>\n",
       "      <td>2020-06-20</td>\n",
       "      <td>black lives matter is aggressively promoting i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>Mike Hulett</td>\n",
       "      <td>2020-06-20</td>\n",
       "      <td>mainstream media have shown white protesters, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>Mike Hulett</td>\n",
       "      <td>2020-07-18</td>\n",
       "      <td>“systemic police racism,” a volatile black liv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>Mike Hulett</td>\n",
       "      <td>2020-08-01</td>\n",
       "      <td>today, many well-meaning americans, peacefully...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>Mike Hulett</td>\n",
       "      <td>2020-08-29</td>\n",
       "      <td>like clockwork, a black lives matter activist ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>Mike Hulett</td>\n",
       "      <td>2020-08-29</td>\n",
       "      <td>understand, when employed as a marxist black l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>Mike Hulett</td>\n",
       "      <td>2020-08-29</td>\n",
       "      <td>nevertheless, straight from the black lives ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>Mike Hulett</td>\n",
       "      <td>2020-08-29</td>\n",
       "      <td>when re-elected, president trump will continue...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>Mike Hulett</td>\n",
       "      <td>2020-10-23</td>\n",
       "      <td>danz repeated other unsubstantiated, slanderou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>Mike Hulett</td>\n",
       "      <td>2020-10-23</td>\n",
       "      <td>how did danz dream up “the militia he’s unleas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>532</th>\n",
       "      <td>Mike Hulett</td>\n",
       "      <td>2021-01-16</td>\n",
       "      <td>some were identified as black lives matter act...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>670</th>\n",
       "      <td>Mike Hulett</td>\n",
       "      <td>2021-03-27</td>\n",
       "      <td>black lives matter was created in part on the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>790</th>\n",
       "      <td>Mike Hulett</td>\n",
       "      <td>2021-06-05</td>\n",
       "      <td>others lift up marxist groups like black lives...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>848</th>\n",
       "      <td>Mike Hulett</td>\n",
       "      <td>2021-07-17</td>\n",
       "      <td>documented black lives matter protests against...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>850</th>\n",
       "      <td>Mike Hulett</td>\n",
       "      <td>2021-07-17</td>\n",
       "      <td>nationwide violence escalated following unrest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1189</th>\n",
       "      <td>Mike Hulett</td>\n",
       "      <td>2022-01-15</td>\n",
       "      <td>readers, these are the same people that silent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1198</th>\n",
       "      <td>Mike Hulett</td>\n",
       "      <td>2022-02-13</td>\n",
       "      <td>black lives matter led the way in attacking la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1199</th>\n",
       "      <td>Mike Hulett</td>\n",
       "      <td>2022-02-13</td>\n",
       "      <td>anyone challenging unlawful behavior by black ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1200</th>\n",
       "      <td>Mike Hulett</td>\n",
       "      <td>2022-02-13</td>\n",
       "      <td>black lives matter has since been revealed to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1201</th>\n",
       "      <td>Mike Hulett</td>\n",
       "      <td>2022-02-13</td>\n",
       "      <td>three self-proclaimed marxist founders have de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1409</th>\n",
       "      <td>Mike Hulett</td>\n",
       "      <td>2022-05-21</td>\n",
       "      <td>while breathlessly buttressing the democrats' ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1501</th>\n",
       "      <td>Mike Hulett</td>\n",
       "      <td>2022-07-30</td>\n",
       "      <td>leftist antifa thugs and black lives matter ri...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        columnist  file_date  \\\n",
       "130   Mike Hulett 2020-06-20   \n",
       "132   Mike Hulett 2020-06-20   \n",
       "212   Mike Hulett 2020-07-18   \n",
       "240   Mike Hulett 2020-08-01   \n",
       "281   Mike Hulett 2020-08-29   \n",
       "284   Mike Hulett 2020-08-29   \n",
       "288   Mike Hulett 2020-08-29   \n",
       "291   Mike Hulett 2020-08-29   \n",
       "375   Mike Hulett 2020-10-23   \n",
       "380   Mike Hulett 2020-10-23   \n",
       "532   Mike Hulett 2021-01-16   \n",
       "670   Mike Hulett 2021-03-27   \n",
       "790   Mike Hulett 2021-06-05   \n",
       "848   Mike Hulett 2021-07-17   \n",
       "850   Mike Hulett 2021-07-17   \n",
       "1189  Mike Hulett 2022-01-15   \n",
       "1198  Mike Hulett 2022-02-13   \n",
       "1199  Mike Hulett 2022-02-13   \n",
       "1200  Mike Hulett 2022-02-13   \n",
       "1201  Mike Hulett 2022-02-13   \n",
       "1409  Mike Hulett 2022-05-21   \n",
       "1501  Mike Hulett 2022-07-30   \n",
       "\n",
       "                                                   text  \n",
       "130   black lives matter is aggressively promoting i...  \n",
       "132   mainstream media have shown white protesters, ...  \n",
       "212   “systemic police racism,” a volatile black liv...  \n",
       "240   today, many well-meaning americans, peacefully...  \n",
       "281   like clockwork, a black lives matter activist ...  \n",
       "284   understand, when employed as a marxist black l...  \n",
       "288   nevertheless, straight from the black lives ma...  \n",
       "291   when re-elected, president trump will continue...  \n",
       "375   danz repeated other unsubstantiated, slanderou...  \n",
       "380   how did danz dream up “the militia he’s unleas...  \n",
       "532   some were identified as black lives matter act...  \n",
       "670   black lives matter was created in part on the ...  \n",
       "790   others lift up marxist groups like black lives...  \n",
       "848   documented black lives matter protests against...  \n",
       "850   nationwide violence escalated following unrest...  \n",
       "1189  readers, these are the same people that silent...  \n",
       "1198  black lives matter led the way in attacking la...  \n",
       "1199  anyone challenging unlawful behavior by black ...  \n",
       "1200  black lives matter has since been revealed to ...  \n",
       "1201  three self-proclaimed marxist founders have de...  \n",
       "1409  while breathlessly buttressing the democrats' ...  \n",
       "1501  leftist antifa thugs and black lives matter ri...  "
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blm_sentences =  df_sentences[df_sentences['text'].str.contains('black lives matter')]\n",
    "blm_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>columnist</th>\n",
       "      <th>file_date</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>Mike Hulett</td>\n",
       "      <td>2020-07-04</td>\n",
       "      <td>organized marxist anarchists have spread out a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>Mike Hulett</td>\n",
       "      <td>2020-07-04</td>\n",
       "      <td>armed marxist militias are tearing down artifa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>Mike Hulett</td>\n",
       "      <td>2020-07-04</td>\n",
       "      <td>clearly intended to expunge important chronicl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>Mike Hulett</td>\n",
       "      <td>2020-07-04</td>\n",
       "      <td>sadly, liberals are unwittingly collaborating ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>Mike Hulett</td>\n",
       "      <td>2020-07-04</td>\n",
       "      <td>or, conversely, will america be judged by vote...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>Mike Hulett</td>\n",
       "      <td>2020-08-01</td>\n",
       "      <td>today, many well-meaning americans, peacefully...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>Mike Hulett</td>\n",
       "      <td>2020-08-01</td>\n",
       "      <td>mainstream media describe nightly marxist prot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>Mike Hulett</td>\n",
       "      <td>2020-08-01</td>\n",
       "      <td>intentional marxist attacks on united states f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>Mike Hulett</td>\n",
       "      <td>2020-08-29</td>\n",
       "      <td>understand, when employed as a marxist black l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>Mike Hulett</td>\n",
       "      <td>2020-08-29</td>\n",
       "      <td>when re-elected, president trump will continue...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>Mike Hulett</td>\n",
       "      <td>2020-10-23</td>\n",
       "      <td>how did danz dream up “the militia he’s unleas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>Mike Hulett</td>\n",
       "      <td>2020-12-19</td>\n",
       "      <td>privileged democrat elites, blm marxists, and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>Mike Hulett</td>\n",
       "      <td>2020-12-19</td>\n",
       "      <td>behold the real war on democracy: marxist-soci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>735</th>\n",
       "      <td>Mike Hulett</td>\n",
       "      <td>2021-04-24</td>\n",
       "      <td>congress, courts and states must act immediate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>751</th>\n",
       "      <td>Mike Hulett</td>\n",
       "      <td>2021-05-08</td>\n",
       "      <td>“in this crisis,” the majority of americans wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>790</th>\n",
       "      <td>Mike Hulett</td>\n",
       "      <td>2021-06-05</td>\n",
       "      <td>others lift up marxist groups like black lives...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>Mike Hulett</td>\n",
       "      <td>2021-06-19</td>\n",
       "      <td>lt. col. matthew lohmeier, united states space...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800</th>\n",
       "      <td>Mike Hulett</td>\n",
       "      <td>2021-06-19</td>\n",
       "      <td>and i had recognized those narratives as being...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>808</th>\n",
       "      <td>Mike Hulett</td>\n",
       "      <td>2021-06-19</td>\n",
       "      <td>those employees are forced to listen to marxis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>964</th>\n",
       "      <td>Mike Hulett</td>\n",
       "      <td>2021-09-11</td>\n",
       "      <td>in may came details on biden’s marxist handler...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>Mike Hulett</td>\n",
       "      <td>2021-09-25</td>\n",
       "      <td>today, republicans are haplessly wandering aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1200</th>\n",
       "      <td>Mike Hulett</td>\n",
       "      <td>2022-02-13</td>\n",
       "      <td>black lives matter has since been revealed to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1201</th>\n",
       "      <td>Mike Hulett</td>\n",
       "      <td>2022-02-13</td>\n",
       "      <td>three self-proclaimed marxist founders have de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1583</th>\n",
       "      <td>Mike Hulett</td>\n",
       "      <td>2022-09-10</td>\n",
       "      <td>record inflation, outrageous prices, borders c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1670</th>\n",
       "      <td>Mike Hulett</td>\n",
       "      <td>2022-10-22</td>\n",
       "      <td>ruling america are power-hungry progressives, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        columnist  file_date  \\\n",
       "159   Mike Hulett 2020-07-04   \n",
       "168   Mike Hulett 2020-07-04   \n",
       "170   Mike Hulett 2020-07-04   \n",
       "172   Mike Hulett 2020-07-04   \n",
       "177   Mike Hulett 2020-07-04   \n",
       "240   Mike Hulett 2020-08-01   \n",
       "242   Mike Hulett 2020-08-01   \n",
       "243   Mike Hulett 2020-08-01   \n",
       "284   Mike Hulett 2020-08-29   \n",
       "291   Mike Hulett 2020-08-29   \n",
       "380   Mike Hulett 2020-10-23   \n",
       "491   Mike Hulett 2020-12-19   \n",
       "492   Mike Hulett 2020-12-19   \n",
       "735   Mike Hulett 2021-04-24   \n",
       "751   Mike Hulett 2021-05-08   \n",
       "790   Mike Hulett 2021-06-05   \n",
       "795   Mike Hulett 2021-06-19   \n",
       "800   Mike Hulett 2021-06-19   \n",
       "808   Mike Hulett 2021-06-19   \n",
       "964   Mike Hulett 2021-09-11   \n",
       "995   Mike Hulett 2021-09-25   \n",
       "1200  Mike Hulett 2022-02-13   \n",
       "1201  Mike Hulett 2022-02-13   \n",
       "1583  Mike Hulett 2022-09-10   \n",
       "1670  Mike Hulett 2022-10-22   \n",
       "\n",
       "                                                   text  \n",
       "159   organized marxist anarchists have spread out a...  \n",
       "168   armed marxist militias are tearing down artifa...  \n",
       "170   clearly intended to expunge important chronicl...  \n",
       "172   sadly, liberals are unwittingly collaborating ...  \n",
       "177   or, conversely, will america be judged by vote...  \n",
       "240   today, many well-meaning americans, peacefully...  \n",
       "242   mainstream media describe nightly marxist prot...  \n",
       "243   intentional marxist attacks on united states f...  \n",
       "284   understand, when employed as a marxist black l...  \n",
       "291   when re-elected, president trump will continue...  \n",
       "380   how did danz dream up “the militia he’s unleas...  \n",
       "491   privileged democrat elites, blm marxists, and ...  \n",
       "492   behold the real war on democracy: marxist-soci...  \n",
       "735   congress, courts and states must act immediate...  \n",
       "751   “in this crisis,” the majority of americans wi...  \n",
       "790   others lift up marxist groups like black lives...  \n",
       "795   lt. col. matthew lohmeier, united states space...  \n",
       "800   and i had recognized those narratives as being...  \n",
       "808   those employees are forced to listen to marxis...  \n",
       "964   in may came details on biden’s marxist handler...  \n",
       "995   today, republicans are haplessly wandering aro...  \n",
       "1200  black lives matter has since been revealed to ...  \n",
       "1201  three self-proclaimed marxist founders have de...  \n",
       "1583  record inflation, outrageous prices, borders c...  \n",
       "1670  ruling america are power-hungry progressives, ...  "
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "marxist_sentences =  df_sentences[df_sentences['text'].str.contains('marx')]\n",
    "marxist_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_contains_term = 0\n",
    "term = 'black lives matter'\n",
    "# corpus_length = 0\n",
    "  \n",
    "term_docs = count_docs_contain_term(term)\n",
    "\n",
    "for i in range(len(columnist_files)):\n",
    "    file = columnist_files[i]\n",
    "    s = read_text_file(file)\n",
    "    cleaned_string = clean_string(s)\n",
    "    td_idf = cleaned_string.count(term) / count_words(s) * term_docs\n",
    "    print('document', i, 'tdidf', td_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bigram</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13903</th>\n",
       "      <td>(president, trump)</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2863</th>\n",
       "      <td>(joe, biden)</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7588</th>\n",
       "      <td>(mainstream, media)</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8531</th>\n",
       "      <td>(vice, president)</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16713</th>\n",
       "      <td>(black, lives)</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7750</th>\n",
       "      <td>(white, house)</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15733</th>\n",
       "      <td>(lives, matter)</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3567</th>\n",
       "      <td>(donald, trump)</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16557</th>\n",
       "      <td>(law, enforcement)</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18904</th>\n",
       "      <td>(border, patrol)</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    bigram  count\n",
       "13903   (president, trump)     33\n",
       "2863          (joe, biden)     32\n",
       "7588   (mainstream, media)     30\n",
       "8531     (vice, president)     23\n",
       "16713       (black, lives)     22\n",
       "7750        (white, house)     21\n",
       "15733      (lives, matter)     21\n",
       "3567       (donald, trump)     19\n",
       "16557   (law, enforcement)     19\n",
       "18904     (border, patrol)     18"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bigrams.sort_values(by='count', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>columnist</th>\n",
       "      <th>file_date</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>Mike Hulett</td>\n",
       "      <td>2020-06-20</td>\n",
       "      <td>personal note: i have known hundreds of dedica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>707</th>\n",
       "      <td>Mike Hulett</td>\n",
       "      <td>2021-04-24</td>\n",
       "      <td>our border patrol van turned onto south sasabe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>716</th>\n",
       "      <td>Mike Hulett</td>\n",
       "      <td>2021-04-24</td>\n",
       "      <td>our border patrol instructors described how ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>718</th>\n",
       "      <td>Mike Hulett</td>\n",
       "      <td>2021-04-24</td>\n",
       "      <td>during scorching hot summer months, border pat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>734</th>\n",
       "      <td>Mike Hulett</td>\n",
       "      <td>2021-04-24</td>\n",
       "      <td>just as tucson police officers under magnus re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1020</th>\n",
       "      <td>Mike Hulett</td>\n",
       "      <td>2021-10-09</td>\n",
       "      <td>vice president kamala harris was “outraged” ov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1022</th>\n",
       "      <td>Mike Hulett</td>\n",
       "      <td>2021-10-09</td>\n",
       "      <td>president joe biden angrily declared border pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1023</th>\n",
       "      <td>Mike Hulett</td>\n",
       "      <td>2021-10-09</td>\n",
       "      <td>the truth is, border patrol agents don’t carry...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1025</th>\n",
       "      <td>Mike Hulett</td>\n",
       "      <td>2021-10-09</td>\n",
       "      <td>in the border patrol citizen academy, we learn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1027</th>\n",
       "      <td>Mike Hulett</td>\n",
       "      <td>2021-10-09</td>\n",
       "      <td>the border patrol's horseback \"samaritans\" wou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1044</th>\n",
       "      <td>Mike Hulett</td>\n",
       "      <td>2021-10-23</td>\n",
       "      <td>rodney s. scott served as a u.s. customs and b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1045</th>\n",
       "      <td>Mike Hulett</td>\n",
       "      <td>2021-10-23</td>\n",
       "      <td>he rose through the ranks to be named chief of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1054</th>\n",
       "      <td>Mike Hulett</td>\n",
       "      <td>2021-10-23</td>\n",
       "      <td>border patrol, ice and dhs employees are eager...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1221</th>\n",
       "      <td>Mike Hulett</td>\n",
       "      <td>2022-02-26</td>\n",
       "      <td>columnist angie wong wrote in the new york pos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1223</th>\n",
       "      <td>Mike Hulett</td>\n",
       "      <td>2022-02-26</td>\n",
       "      <td>so the border patrol has made deals with the d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1239</th>\n",
       "      <td>Mike Hulett</td>\n",
       "      <td>2022-02-26</td>\n",
       "      <td>border patrol agents led my 2019 citizen acade...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1244</th>\n",
       "      <td>Mike Hulett</td>\n",
       "      <td>2022-02-26</td>\n",
       "      <td>border patrol pretends to patrol the border.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1709</th>\n",
       "      <td>Mike Hulett</td>\n",
       "      <td>2022-11-19</td>\n",
       "      <td>a “far-right echo chamber”\\nalong with his psy...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        columnist  file_date  \\\n",
       "148   Mike Hulett 2020-06-20   \n",
       "707   Mike Hulett 2021-04-24   \n",
       "716   Mike Hulett 2021-04-24   \n",
       "718   Mike Hulett 2021-04-24   \n",
       "734   Mike Hulett 2021-04-24   \n",
       "1020  Mike Hulett 2021-10-09   \n",
       "1022  Mike Hulett 2021-10-09   \n",
       "1023  Mike Hulett 2021-10-09   \n",
       "1025  Mike Hulett 2021-10-09   \n",
       "1027  Mike Hulett 2021-10-09   \n",
       "1044  Mike Hulett 2021-10-23   \n",
       "1045  Mike Hulett 2021-10-23   \n",
       "1054  Mike Hulett 2021-10-23   \n",
       "1221  Mike Hulett 2022-02-26   \n",
       "1223  Mike Hulett 2022-02-26   \n",
       "1239  Mike Hulett 2022-02-26   \n",
       "1244  Mike Hulett 2022-02-26   \n",
       "1709  Mike Hulett 2022-11-19   \n",
       "\n",
       "                                                   text  \n",
       "148   personal note: i have known hundreds of dedica...  \n",
       "707   our border patrol van turned onto south sasabe...  \n",
       "716   our border patrol instructors described how ca...  \n",
       "718   during scorching hot summer months, border pat...  \n",
       "734   just as tucson police officers under magnus re...  \n",
       "1020  vice president kamala harris was “outraged” ov...  \n",
       "1022  president joe biden angrily declared border pa...  \n",
       "1023  the truth is, border patrol agents don’t carry...  \n",
       "1025  in the border patrol citizen academy, we learn...  \n",
       "1027  the border patrol's horseback \"samaritans\" wou...  \n",
       "1044  rodney s. scott served as a u.s. customs and b...  \n",
       "1045  he rose through the ranks to be named chief of...  \n",
       "1054  border patrol, ice and dhs employees are eager...  \n",
       "1221  columnist angie wong wrote in the new york pos...  \n",
       "1223  so the border patrol has made deals with the d...  \n",
       "1239  border patrol agents led my 2019 citizen acade...  \n",
       "1244       border patrol pretends to patrol the border.  \n",
       "1709  a “far-right echo chamber”\\nalong with his psy...  "
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "media_sentences = df_sentences[df_sentences['text'].str.contains('mainstream media')]\n",
    "media_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_words.sort_values(by='count', ascending=False).head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document 0 tdidf 0.058637083993660855\n",
      "document 1 tdidf 0.0\n",
      "document 2 tdidf 0.0\n",
      "document 3 tdidf 0.7808441558441559\n",
      "document 4 tdidf 0.0\n",
      "document 5 tdidf 0.061872909698996656\n",
      "document 6 tdidf 0.0\n",
      "document 7 tdidf 0.0\n",
      "document 8 tdidf 0.49087893864013266\n",
      "document 9 tdidf 0.25170068027210885\n",
      "document 10 tdidf 0.06379310344827586\n",
      "document 11 tdidf 0.0\n",
      "document 12 tdidf 0.8946459412780657\n",
      "document 13 tdidf 0.7474747474747475\n",
      "document 14 tdidf 0.06357388316151202\n",
      "document 15 tdidf 0.5522388059701493\n",
      "document 16 tdidf 0.30833333333333335\n",
      "document 17 tdidf 0.12171052631578946\n",
      "document 18 tdidf 0.30377668308702793\n",
      "document 19 tdidf 0.13167259786476868\n",
      "document 20 tdidf 0.36333878887070375\n",
      "document 21 tdidf 1.2993311036789297\n",
      "document 22 tdidf 0.0\n",
      "document 23 tdidf 0.18813559322033896\n",
      "document 24 tdidf 0.0\n",
      "document 25 tdidf 0.0\n",
      "document 26 tdidf 0.06468531468531469\n",
      "document 27 tdidf 0.0\n",
      "document 28 tdidf 0.0\n",
      "document 29 tdidf 0.0\n",
      "document 30 tdidf 0.3130287648054145\n",
      "document 31 tdidf 0.43676222596964587\n",
      "document 32 tdidf 0.0\n",
      "document 33 tdidf 0.0\n",
      "document 34 tdidf 0.12736660929432014\n",
      "document 35 tdidf 0.0\n",
      "document 36 tdidf 0.0\n",
      "document 37 tdidf 0.2483221476510067\n",
      "document 38 tdidf 0.0\n",
      "document 39 tdidf 0.0\n",
      "document 40 tdidf 0.0642361111111111\n",
      "document 41 tdidf 0.06390328151986183\n",
      "document 42 tdidf 0.0\n",
      "document 43 tdidf 0.5568561872909699\n",
      "document 44 tdidf 0.1878172588832487\n",
      "document 45 tdidf 0.0\n",
      "document 46 tdidf 0.0\n",
      "document 47 tdidf 0.0\n",
      "document 48 tdidf 0.0\n",
      "document 49 tdidf 0.0\n",
      "document 50 tdidf 0.12071778140293639\n",
      "document 51 tdidf 0.3125\n",
      "document 52 tdidf 0.0\n",
      "document 53 tdidf 0.7326732673267327\n",
      "document 54 tdidf 0.0\n",
      "document 55 tdidf 0.0\n",
      "document 56 tdidf 0.5568561872909699\n",
      "document 57 tdidf 0.46167557932263814\n",
      "document 58 tdidf 0.3881118881118881\n",
      "document 59 tdidf 0.0\n",
      "document 60 tdidf 0.12416107382550336\n",
      "document 61 tdidf 0.0\n",
      "document 62 tdidf 0.756388415672913\n",
      "document 63 tdidf 0.3881118881118881\n",
      "document 64 tdidf 0.1269296740994854\n",
      "document 65 tdidf 0.3769100169779287\n",
      "document 66 tdidf 0.0\n",
      "document 67 tdidf 0.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "biden_sentences =  df_sentences[df_sentences['text'].str.contains('biden')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>columnist</th>\n",
       "      <th>file_date</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mike Hulett</td>\n",
       "      <td>2020-04-11</td>\n",
       "      <td>kamala harris, d-calif., and cory booker, d-n....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mike Hulett</td>\n",
       "      <td>2020-04-11</td>\n",
       "      <td>biden, of course, is the friendly old politici...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Mike Hulett</td>\n",
       "      <td>2020-04-11</td>\n",
       "      <td>apparently that is deemed a small price for pu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Mike Hulett</td>\n",
       "      <td>2020-04-11</td>\n",
       "      <td>actually, one significant problem involves for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Mike Hulett</td>\n",
       "      <td>2020-04-11</td>\n",
       "      <td>shokin has successfully secured a ukraine cour...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1674</th>\n",
       "      <td>Mike Hulett</td>\n",
       "      <td>2022-11-05</td>\n",
       "      <td>since biden took office, over 2.3 million immi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1688</th>\n",
       "      <td>Mike Hulett</td>\n",
       "      <td>2022-11-05</td>\n",
       "      <td>retired four-star general jack keane warns us ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1691</th>\n",
       "      <td>Mike Hulett</td>\n",
       "      <td>2022-11-05</td>\n",
       "      <td>this column has frequently delineated biden’s ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1692</th>\n",
       "      <td>Mike Hulett</td>\n",
       "      <td>2022-11-05</td>\n",
       "      <td>we know brother jim and son hunter enriched th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1693</th>\n",
       "      <td>Mike Hulett</td>\n",
       "      <td>2022-11-05</td>\n",
       "      <td>congressional democrats would impeach and conv...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        columnist  file_date  \\\n",
       "3     Mike Hulett 2020-04-11   \n",
       "4     Mike Hulett 2020-04-11   \n",
       "6     Mike Hulett 2020-04-11   \n",
       "12    Mike Hulett 2020-04-11   \n",
       "13    Mike Hulett 2020-04-11   \n",
       "...           ...        ...   \n",
       "1674  Mike Hulett 2022-11-05   \n",
       "1688  Mike Hulett 2022-11-05   \n",
       "1691  Mike Hulett 2022-11-05   \n",
       "1692  Mike Hulett 2022-11-05   \n",
       "1693  Mike Hulett 2022-11-05   \n",
       "\n",
       "                                                   text  \n",
       "3     kamala harris, d-calif., and cory booker, d-n....  \n",
       "4     biden, of course, is the friendly old politici...  \n",
       "6     apparently that is deemed a small price for pu...  \n",
       "12    actually, one significant problem involves for...  \n",
       "13    shokin has successfully secured a ukraine cour...  \n",
       "...                                                 ...  \n",
       "1674  since biden took office, over 2.3 million immi...  \n",
       "1688  retired four-star general jack keane warns us ...  \n",
       "1691  this column has frequently delineated biden’s ...  \n",
       "1692  we know brother jim and son hunter enriched th...  \n",
       "1693  congressional democrats would impeach and conv...  \n",
       "\n",
       "[200 rows x 3 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biden_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(df_vocab.distinct_words.to_list()[0])\n",
    "hulett = sorted(df_vocab.distinct_words_excl_stop.to_list()[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\SAND8464\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['abandon',\n",
       " 'abandoned',\n",
       " 'abandoning',\n",
       " 'abc',\n",
       " 'abetted',\n",
       " 'abide',\n",
       " 'abiding',\n",
       " 'ability',\n",
       " 'abjectly',\n",
       " 'able',\n",
       " 'abnormal',\n",
       " 'abolish',\n",
       " 'abolishing',\n",
       " 'abortion',\n",
       " 'abound',\n",
       " 'abraham',\n",
       " 'abrams',\n",
       " 'abroad',\n",
       " 'abrupt',\n",
       " 'abruptly',\n",
       " 'absent',\n",
       " 'absolute',\n",
       " 'absolutely',\n",
       " 'abundance',\n",
       " 'abundant',\n",
       " 'abuse',\n",
       " 'academia',\n",
       " 'academy',\n",
       " 'acceleration',\n",
       " 'accept',\n",
       " 'acceptable',\n",
       " 'acceptance',\n",
       " 'accepted',\n",
       " 'accepting',\n",
       " 'access',\n",
       " 'accident',\n",
       " 'accomplish',\n",
       " 'accomplished',\n",
       " 'accomplishment',\n",
       " 'according',\n",
       " 'accordingly',\n",
       " 'account',\n",
       " 'accountability',\n",
       " 'accredited',\n",
       " 'accurate',\n",
       " 'accurately',\n",
       " 'accusation',\n",
       " 'accused',\n",
       " 'accuser',\n",
       " 'achieve',\n",
       " 'achieved',\n",
       " 'achievement',\n",
       " 'acknowledge',\n",
       " 'acknowledged',\n",
       " 'acquire',\n",
       " 'acquittal',\n",
       " 'across',\n",
       " 'act',\n",
       " 'acted',\n",
       " 'acting',\n",
       " 'action',\n",
       " 'active',\n",
       " 'activist',\n",
       " 'activity',\n",
       " 'actor',\n",
       " 'actual',\n",
       " 'actually',\n",
       " 'ad',\n",
       " 'adam',\n",
       " 'add',\n",
       " 'added',\n",
       " 'addiction',\n",
       " 'adding',\n",
       " 'addition',\n",
       " 'additional',\n",
       " 'addled',\n",
       " 'address',\n",
       " 'addressed',\n",
       " 'addressing',\n",
       " 'adjudicate',\n",
       " 'adjudicated',\n",
       " 'administer',\n",
       " 'administration',\n",
       " 'administrative',\n",
       " 'admired',\n",
       " 'admires',\n",
       " 'admission',\n",
       " 'admits',\n",
       " 'admitted',\n",
       " 'admitting',\n",
       " 'adoration',\n",
       " 'adult',\n",
       " 'advance',\n",
       " 'advanced',\n",
       " 'advantage',\n",
       " 'adversary',\n",
       " 'adverse',\n",
       " 'adversely',\n",
       " 'advertising',\n",
       " 'advice',\n",
       " 'advised',\n",
       " 'advocacy',\n",
       " 'advocate',\n",
       " 'advocated',\n",
       " 'aerial',\n",
       " 'aerobatics',\n",
       " 'aerosports',\n",
       " 'affair',\n",
       " 'affecting',\n",
       " 'affidavit',\n",
       " 'affiliation',\n",
       " 'affirmed',\n",
       " 'afghan',\n",
       " 'afghanistan',\n",
       " 'afraid',\n",
       " 'africa',\n",
       " 'african',\n",
       " 'aftermath',\n",
       " 'aftershock',\n",
       " 'age',\n",
       " 'aged',\n",
       " 'agency',\n",
       " 'agenda',\n",
       " 'agent',\n",
       " 'aggravated',\n",
       " 'aggression',\n",
       " 'aggressively',\n",
       " 'ago',\n",
       " 'agree',\n",
       " 'agreed',\n",
       " 'ah',\n",
       " 'ahead',\n",
       " 'aid',\n",
       " 'aide',\n",
       " 'aim',\n",
       " 'aimed',\n",
       " 'aint',\n",
       " 'air',\n",
       " 'aircraft',\n",
       " 'airline',\n",
       " 'airplane',\n",
       " 'airport',\n",
       " 'airsho',\n",
       " 'airshows',\n",
       " 'al',\n",
       " 'alabama',\n",
       " 'alan',\n",
       " 'alarm',\n",
       " 'alarmed',\n",
       " 'alarming',\n",
       " 'alarmingly',\n",
       " 'alarmist',\n",
       " 'alejandro',\n",
       " 'alert',\n",
       " 'alexandria',\n",
       " 'alice',\n",
       " 'alicia',\n",
       " 'alien',\n",
       " 'aligned',\n",
       " 'alike',\n",
       " 'alison',\n",
       " 'allegation',\n",
       " 'allege',\n",
       " 'alleged',\n",
       " 'allegedly',\n",
       " 'alleging',\n",
       " 'allow',\n",
       " 'allowed',\n",
       " 'allowing',\n",
       " 'almost',\n",
       " 'alone',\n",
       " 'along',\n",
       " 'alongside',\n",
       " 'already',\n",
       " 'also',\n",
       " 'alternative',\n",
       " 'although',\n",
       " 'always',\n",
       " 'amateur',\n",
       " 'amazing',\n",
       " 'amazingly',\n",
       " 'ambassador',\n",
       " 'amendment',\n",
       " 'amenity',\n",
       " 'america',\n",
       " 'american',\n",
       " 'amid',\n",
       " 'among',\n",
       " 'amphitheater',\n",
       " 'ample',\n",
       " 'amply',\n",
       " 'amusing',\n",
       " 'amy',\n",
       " 'analogous',\n",
       " 'analysis',\n",
       " 'analyst',\n",
       " 'anarchist',\n",
       " 'anarchy',\n",
       " 'anchor',\n",
       " 'anderson',\n",
       " 'andor',\n",
       " 'andrew',\n",
       " 'angel',\n",
       " 'angeles',\n",
       " 'angie',\n",
       " 'angle',\n",
       " 'angrily',\n",
       " 'angry',\n",
       " 'annihilating',\n",
       " 'annihilation',\n",
       " 'anniversary',\n",
       " 'announced',\n",
       " 'annual',\n",
       " 'annually',\n",
       " 'anointed',\n",
       " 'anonymous',\n",
       " 'another',\n",
       " 'answer',\n",
       " 'answerable',\n",
       " 'antagonist',\n",
       " 'anthem',\n",
       " 'anti',\n",
       " 'antifa',\n",
       " 'antifablm',\n",
       " 'antiquated',\n",
       " 'antiracist',\n",
       " 'anxiety',\n",
       " 'anxious',\n",
       " 'anxiously',\n",
       " 'anybody',\n",
       " 'anymore',\n",
       " 'anyone',\n",
       " 'anything',\n",
       " 'anyway',\n",
       " 'anywhere',\n",
       " 'apart',\n",
       " 'apartheid',\n",
       " 'apartment',\n",
       " 'apologize',\n",
       " 'apologized',\n",
       " 'apology',\n",
       " 'appalling',\n",
       " 'apparent',\n",
       " 'apparently',\n",
       " 'appear',\n",
       " 'appearance',\n",
       " 'appeared',\n",
       " 'appearing',\n",
       " 'appears',\n",
       " 'applaud',\n",
       " 'apple',\n",
       " 'application',\n",
       " 'applied',\n",
       " 'apply',\n",
       " 'applying',\n",
       " 'appoint',\n",
       " 'appointed',\n",
       " 'appointee',\n",
       " 'appreciate',\n",
       " 'apprehended',\n",
       " 'apprehension',\n",
       " 'approach',\n",
       " 'approaching',\n",
       " 'appropriate',\n",
       " 'approval',\n",
       " 'approved',\n",
       " 'approximately',\n",
       " 'april',\n",
       " 'arabia',\n",
       " 'arbiter',\n",
       " 'arbitrarily',\n",
       " 'arbitrary',\n",
       " 'arch',\n",
       " 'archenemy',\n",
       " 'architect',\n",
       " 'archive',\n",
       " 'area',\n",
       " 'arent',\n",
       " 'arguably',\n",
       " 'argues',\n",
       " 'arguing',\n",
       " 'argument',\n",
       " 'ariz',\n",
       " 'arizona',\n",
       " 'ark',\n",
       " 'arkansas',\n",
       " 'arm',\n",
       " 'armed',\n",
       " 'armor',\n",
       " 'armstrong',\n",
       " 'army',\n",
       " 'arnd',\n",
       " 'around',\n",
       " 'arrange',\n",
       " 'array',\n",
       " 'arrest',\n",
       " 'arrested',\n",
       " 'arrived',\n",
       " 'arriving',\n",
       " 'art',\n",
       " 'artfully',\n",
       " 'article',\n",
       " 'articulate',\n",
       " 'articulated',\n",
       " 'artifact',\n",
       " 'artist',\n",
       " 'ash',\n",
       " 'ashli',\n",
       " 'asian',\n",
       " 'aside',\n",
       " 'ask',\n",
       " 'asked',\n",
       " 'asking',\n",
       " 'aspect',\n",
       " 'assassinated',\n",
       " 'assault',\n",
       " 'assaulted',\n",
       " 'assaulting',\n",
       " 'assembling',\n",
       " 'assembly',\n",
       " 'assertion',\n",
       " 'assessment',\n",
       " 'assigned',\n",
       " 'assignment',\n",
       " 'assimilating',\n",
       " 'assist',\n",
       " 'assistance',\n",
       " 'assistant',\n",
       " 'assisted',\n",
       " 'associated',\n",
       " 'association',\n",
       " 'assortment',\n",
       " 'assurance',\n",
       " 'assure',\n",
       " 'assured',\n",
       " 'assuredly',\n",
       " 'astonished',\n",
       " 'astonishing',\n",
       " 'astonishingly',\n",
       " 'astounding',\n",
       " 'asylum',\n",
       " 'athletic',\n",
       " 'athletics',\n",
       " 'atlanta',\n",
       " 'atmosphere',\n",
       " 'attack',\n",
       " 'attacked',\n",
       " 'attacker',\n",
       " 'attacking',\n",
       " 'attempt',\n",
       " 'attempted',\n",
       " 'attempting',\n",
       " 'attend',\n",
       " 'attendee',\n",
       " 'attention',\n",
       " 'attorney',\n",
       " 'attract',\n",
       " 'attractive',\n",
       " 'attributed',\n",
       " 'attribution',\n",
       " 'audacity',\n",
       " 'audience',\n",
       " 'auditorium',\n",
       " 'aug',\n",
       " 'august',\n",
       " 'authentic',\n",
       " 'authoritarian',\n",
       " 'authority',\n",
       " 'authorization',\n",
       " 'authorizes',\n",
       " 'automatic',\n",
       " 'automotive',\n",
       " 'autonomous',\n",
       " 'available',\n",
       " 'avenue',\n",
       " 'avenuecenter',\n",
       " 'average',\n",
       " 'avert',\n",
       " 'averted',\n",
       " 'averting',\n",
       " 'aviator',\n",
       " 'avic',\n",
       " 'avoid',\n",
       " 'avoidable',\n",
       " 'avoided',\n",
       " 'avowed',\n",
       " 'award',\n",
       " 'awarded',\n",
       " 'aware',\n",
       " 'awareness',\n",
       " 'away',\n",
       " 'aways',\n",
       " 'awe',\n",
       " 'awkward',\n",
       " 'b',\n",
       " 'babbitt',\n",
       " 'baby',\n",
       " 'back',\n",
       " 'backdrop',\n",
       " 'backed',\n",
       " 'background',\n",
       " 'backhandedly',\n",
       " 'backing',\n",
       " 'backwards',\n",
       " 'bad',\n",
       " 'badge',\n",
       " 'bag',\n",
       " 'bail',\n",
       " 'baiters',\n",
       " 'baiting',\n",
       " 'balanced',\n",
       " 'ballot',\n",
       " 'ban',\n",
       " 'banana',\n",
       " 'band',\n",
       " 'banned',\n",
       " 'banner',\n",
       " 'banning',\n",
       " 'bar',\n",
       " 'barack',\n",
       " 'barden',\n",
       " 'bargain',\n",
       " 'barn',\n",
       " 'barrett',\n",
       " 'barricaded',\n",
       " 'barrier',\n",
       " 'base',\n",
       " 'baseball',\n",
       " 'based',\n",
       " 'bash',\n",
       " 'basic',\n",
       " 'basis',\n",
       " 'bat',\n",
       " 'bathroom',\n",
       " 'battle',\n",
       " 'battleground',\n",
       " 'beach',\n",
       " 'beacon',\n",
       " 'beamed',\n",
       " 'bean',\n",
       " 'bear',\n",
       " 'bearer',\n",
       " 'beaten',\n",
       " 'beating',\n",
       " 'beautiful',\n",
       " 'beauty',\n",
       " 'became',\n",
       " 'become',\n",
       " 'becomes',\n",
       " 'becoming',\n",
       " 'beer',\n",
       " 'befuddled',\n",
       " 'beg',\n",
       " 'began',\n",
       " 'begging',\n",
       " 'begin',\n",
       " 'beginning',\n",
       " 'behavior',\n",
       " 'beheading',\n",
       " 'behind',\n",
       " 'behold',\n",
       " 'beholding',\n",
       " 'being',\n",
       " 'belief',\n",
       " 'believe',\n",
       " 'believed',\n",
       " 'believing',\n",
       " 'bellowed',\n",
       " 'belonging',\n",
       " 'beloved',\n",
       " 'bended',\n",
       " 'bender',\n",
       " 'benefit',\n",
       " 'benefited',\n",
       " 'benevolent',\n",
       " 'benign',\n",
       " 'berlin',\n",
       " 'bernie',\n",
       " 'beside',\n",
       " 'besides',\n",
       " 'best',\n",
       " 'besty',\n",
       " 'bet',\n",
       " 'beto',\n",
       " 'bette',\n",
       " 'better',\n",
       " 'bewildered',\n",
       " 'beyond',\n",
       " 'bias',\n",
       " 'biased',\n",
       " 'bicycle',\n",
       " 'bid',\n",
       " 'biden',\n",
       " 'bidens',\n",
       " 'big',\n",
       " 'bike',\n",
       " 'bikepedestrian',\n",
       " 'biking',\n",
       " 'bilious',\n",
       " 'bill',\n",
       " 'billion',\n",
       " 'billionaire',\n",
       " 'bind',\n",
       " 'binnall',\n",
       " 'binoculars',\n",
       " 'biological',\n",
       " 'birth',\n",
       " 'bison',\n",
       " 'bit',\n",
       " 'bitter',\n",
       " 'bizarre',\n",
       " 'black',\n",
       " 'blackmail',\n",
       " 'blackmailer',\n",
       " 'blacktop',\n",
       " 'blade',\n",
       " 'blame',\n",
       " 'blank',\n",
       " 'blaring',\n",
       " 'blasey',\n",
       " 'blast',\n",
       " 'blatant',\n",
       " 'blatantly',\n",
       " 'bleach',\n",
       " 'bless',\n",
       " 'blessed',\n",
       " 'blessing',\n",
       " 'blighted',\n",
       " 'blind',\n",
       " 'blinder',\n",
       " 'blinding',\n",
       " 'blindly',\n",
       " 'blm',\n",
       " 'block',\n",
       " 'blocked',\n",
       " 'blood',\n",
       " 'bloody',\n",
       " 'bloomberg',\n",
       " 'blow',\n",
       " 'blue',\n",
       " 'bluestem',\n",
       " 'bo',\n",
       " 'board',\n",
       " 'boarded',\n",
       " 'boasted',\n",
       " 'bobby',\n",
       " 'bobulinski',\n",
       " 'bodily',\n",
       " 'body',\n",
       " 'bogus',\n",
       " 'bohai',\n",
       " 'boldly',\n",
       " 'bolt',\n",
       " 'bolton',\n",
       " 'bomb',\n",
       " 'bombast',\n",
       " 'bomber',\n",
       " 'bone',\n",
       " 'book',\n",
       " 'booker',\n",
       " 'booklet',\n",
       " 'boom',\n",
       " 'border',\n",
       " 'bored',\n",
       " 'borgen',\n",
       " 'born',\n",
       " 'bos',\n",
       " 'boss',\n",
       " 'bottle',\n",
       " 'bottom',\n",
       " 'bought',\n",
       " 'boulevard',\n",
       " 'bounty',\n",
       " 'bowser',\n",
       " 'boy',\n",
       " 'boyd',\n",
       " 'bp',\n",
       " 'bracelet',\n",
       " 'bragged',\n",
       " 'brain',\n",
       " 'brainwashed',\n",
       " 'brand',\n",
       " 'branded',\n",
       " 'branding',\n",
       " 'brave',\n",
       " 'brazen',\n",
       " 'brazenly',\n",
       " 'breach',\n",
       " 'break',\n",
       " 'breakfast',\n",
       " 'breaking',\n",
       " 'breath',\n",
       " 'breathlessly',\n",
       " 'breathtakingly',\n",
       " 'brett',\n",
       " 'brian',\n",
       " 'brickner',\n",
       " 'bridge',\n",
       " 'briefing',\n",
       " 'briefly',\n",
       " 'briggs',\n",
       " 'bright',\n",
       " 'bring',\n",
       " 'bringing',\n",
       " 'brings',\n",
       " 'brink',\n",
       " 'britain',\n",
       " 'british',\n",
       " 'broad',\n",
       " 'broadcast',\n",
       " 'broadcaster',\n",
       " 'broader',\n",
       " 'broadly',\n",
       " 'broadway',\n",
       " 'broke',\n",
       " 'broker',\n",
       " 'bronze',\n",
       " 'brother',\n",
       " 'brought',\n",
       " 'brown',\n",
       " 'brutal',\n",
       " 'brutally',\n",
       " 'bubble',\n",
       " 'bucket',\n",
       " 'buddy',\n",
       " 'budget',\n",
       " 'build',\n",
       " 'building',\n",
       " 'built',\n",
       " 'bullied',\n",
       " 'bumbler',\n",
       " 'bummer',\n",
       " 'bumpy',\n",
       " 'bunker',\n",
       " 'bureau',\n",
       " 'bureaucracy',\n",
       " 'burgum',\n",
       " 'buried',\n",
       " 'burisma',\n",
       " 'burn',\n",
       " 'burned',\n",
       " 'burning',\n",
       " 'bury',\n",
       " 'bus',\n",
       " 'bush',\n",
       " 'business',\n",
       " 'bussed',\n",
       " 'busy',\n",
       " 'butcher',\n",
       " 'buttigieg',\n",
       " 'buttressing',\n",
       " 'buy',\n",
       " 'buying',\n",
       " 'buzz',\n",
       " 'byrd',\n",
       " 'byron',\n",
       " 'c',\n",
       " 'ca',\n",
       " 'cabal',\n",
       " 'cabinet',\n",
       " 'cable',\n",
       " 'cactus',\n",
       " 'cadillac',\n",
       " 'cage',\n",
       " 'calculated',\n",
       " 'calendar',\n",
       " 'calif',\n",
       " 'california',\n",
       " 'call',\n",
       " 'called',\n",
       " 'caller',\n",
       " 'calling',\n",
       " 'calm',\n",
       " 'calmly',\n",
       " 'came',\n",
       " 'camera',\n",
       " 'campaign',\n",
       " 'campus',\n",
       " 'canada',\n",
       " 'canadian',\n",
       " 'canard',\n",
       " 'cancel',\n",
       " 'canceled',\n",
       " 'canceling',\n",
       " 'candidacy',\n",
       " 'candidate',\n",
       " 'cant',\n",
       " 'capability',\n",
       " 'capable',\n",
       " 'capacity',\n",
       " 'capitalism',\n",
       " 'capitalist',\n",
       " 'capitol',\n",
       " 'capriciously',\n",
       " 'captive',\n",
       " 'captor',\n",
       " 'car',\n",
       " 'carbon',\n",
       " 'card',\n",
       " 'care',\n",
       " 'cared',\n",
       " 'career',\n",
       " 'careful',\n",
       " 'carefully',\n",
       " 'careless',\n",
       " 'caring',\n",
       " 'carjackers',\n",
       " 'carjacking',\n",
       " 'carnage',\n",
       " 'carolina',\n",
       " 'carrie',\n",
       " 'carried',\n",
       " 'carry',\n",
       " 'carrying',\n",
       " 'cart',\n",
       " 'cartel',\n",
       " 'carved',\n",
       " 'casa',\n",
       " 'case',\n",
       " 'cash',\n",
       " 'casino',\n",
       " 'casket',\n",
       " 'cast',\n",
       " 'casualty',\n",
       " 'cat',\n",
       " 'cataloguing',\n",
       " 'catastrophe',\n",
       " 'catch',\n",
       " 'catchphrase',\n",
       " 'categorically',\n",
       " 'caterwauling',\n",
       " 'caucus',\n",
       " 'caught',\n",
       " 'cause',\n",
       " 'caused',\n",
       " 'causing',\n",
       " 'caution',\n",
       " 'cautious',\n",
       " 'cautiously',\n",
       " 'cbp',\n",
       " 'cbs',\n",
       " 'cease',\n",
       " 'celebrate',\n",
       " 'celebrated',\n",
       " 'celebrating',\n",
       " 'celebration',\n",
       " 'cemetery',\n",
       " 'censored',\n",
       " 'censorship',\n",
       " 'censure',\n",
       " 'cent',\n",
       " 'center',\n",
       " 'centered',\n",
       " 'central',\n",
       " 'centrist',\n",
       " 'century',\n",
       " 'certain',\n",
       " 'certainly',\n",
       " 'certify',\n",
       " 'chai',\n",
       " 'chain',\n",
       " 'chair',\n",
       " 'chaired',\n",
       " 'chalet',\n",
       " 'challenge',\n",
       " 'challenged',\n",
       " 'challenging',\n",
       " 'chamber',\n",
       " 'chance',\n",
       " 'change',\n",
       " 'changed',\n",
       " 'changer',\n",
       " 'changing',\n",
       " 'chanted',\n",
       " 'chanting',\n",
       " 'chaos',\n",
       " 'character',\n",
       " 'characteristic',\n",
       " 'characterize',\n",
       " 'charade',\n",
       " 'charge',\n",
       " 'charged',\n",
       " 'charging',\n",
       " 'charlottesville',\n",
       " 'chasing',\n",
       " 'chasm',\n",
       " 'chatted',\n",
       " 'chauvin',\n",
       " 'chavez',\n",
       " 'cheating',\n",
       " 'check',\n",
       " 'checking',\n",
       " 'cheney',\n",
       " 'chi',\n",
       " 'chicago',\n",
       " 'chief',\n",
       " 'child',\n",
       " 'childhood',\n",
       " 'childish',\n",
       " 'china',\n",
       " 'chinavirus',\n",
       " 'chinese',\n",
       " 'choice',\n",
       " 'choose',\n",
       " 'choosing',\n",
       " 'chose',\n",
       " 'chosen',\n",
       " 'chris',\n",
       " 'christian',\n",
       " 'christine',\n",
       " 'christopher',\n",
       " 'chronicle',\n",
       " 'chronicled',\n",
       " 'chuck',\n",
       " 'church',\n",
       " 'cia',\n",
       " 'cigar',\n",
       " 'circle',\n",
       " 'circling',\n",
       " 'circuit',\n",
       " 'circular',\n",
       " 'circumstance',\n",
       " 'circumvent',\n",
       " 'circumvented',\n",
       " 'circumventing',\n",
       " 'citadel',\n",
       " 'cited',\n",
       " 'citizen',\n",
       " 'citizenship',\n",
       " 'city',\n",
       " 'civic',\n",
       " 'civics',\n",
       " 'civil',\n",
       " 'civilian',\n",
       " 'civilization',\n",
       " 'civilized',\n",
       " 'claim',\n",
       " 'claimed',\n",
       " 'claiming',\n",
       " 'claims—weve',\n",
       " 'claire',\n",
       " 'claptrap',\n",
       " 'clarity',\n",
       " 'class',\n",
       " 'classic',\n",
       " 'classified',\n",
       " 'classmate',\n",
       " 'claudia',\n",
       " 'clay',\n",
       " 'clean',\n",
       " 'cleaning',\n",
       " 'clear',\n",
       " 'clearance',\n",
       " 'cleared',\n",
       " 'clearly',\n",
       " 'click',\n",
       " 'climate',\n",
       " 'clinton',\n",
       " 'clip',\n",
       " 'clockwork',\n",
       " 'close',\n",
       " 'closed',\n",
       " 'closing',\n",
       " 'cloth',\n",
       " 'clothesline',\n",
       " 'clothing',\n",
       " 'club',\n",
       " 'cluster',\n",
       " 'clyburn',\n",
       " 'cmdr',\n",
       " 'cnn',\n",
       " 'co',\n",
       " 'coach',\n",
       " 'coal',\n",
       " 'coast',\n",
       " 'coaster',\n",
       " 'code',\n",
       " 'coded',\n",
       " 'coffee',\n",
       " 'cogent',\n",
       " 'cognitive',\n",
       " 'cognitively',\n",
       " 'cognizant',\n",
       " 'cohort',\n",
       " 'coined',\n",
       " 'col',\n",
       " 'cold',\n",
       " 'collaborating',\n",
       " 'collapse',\n",
       " 'collapsed',\n",
       " 'collapsing',\n",
       " 'colleague',\n",
       " 'collected',\n",
       " 'collection',\n",
       " 'collectively',\n",
       " 'college',\n",
       " 'colluded',\n",
       " 'collusion',\n",
       " 'color',\n",
       " 'colorful',\n",
       " 'colossal',\n",
       " 'columbia',\n",
       " 'column',\n",
       " 'columnist',\n",
       " 'combat',\n",
       " 'combatant',\n",
       " 'combination',\n",
       " 'combined',\n",
       " 'combining',\n",
       " 'combustion',\n",
       " 'come',\n",
       " 'comedian',\n",
       " 'comey',\n",
       " 'comfort',\n",
       " 'comfortable',\n",
       " 'comic',\n",
       " 'coming',\n",
       " 'command',\n",
       " 'commander',\n",
       " 'commanding',\n",
       " 'commemorative',\n",
       " 'commencement',\n",
       " 'comment',\n",
       " 'commentary',\n",
       " 'commentator',\n",
       " 'commerce',\n",
       " 'commercial',\n",
       " 'commission',\n",
       " 'commissioner',\n",
       " 'commit',\n",
       " 'committed',\n",
       " 'committee',\n",
       " 'committing',\n",
       " 'common',\n",
       " 'commonplace',\n",
       " 'commotion',\n",
       " 'comms',\n",
       " 'communicated',\n",
       " 'communist',\n",
       " 'community',\n",
       " 'company',\n",
       " 'compared',\n",
       " 'compelling',\n",
       " 'compensated',\n",
       " 'compete',\n",
       " 'competence',\n",
       " 'competent',\n",
       " 'competing',\n",
       " 'competition',\n",
       " 'compiles',\n",
       " 'complete',\n",
       " 'completed',\n",
       " 'completely',\n",
       " 'complex',\n",
       " 'complicit',\n",
       " 'component',\n",
       " 'comprehension',\n",
       " 'comprehensive',\n",
       " 'compressed',\n",
       " 'computer',\n",
       " 'comrade',\n",
       " 'con',\n",
       " 'concealed',\n",
       " 'concede',\n",
       " 'conceded',\n",
       " 'conceivable',\n",
       " 'conceived',\n",
       " 'conception',\n",
       " 'concern',\n",
       " 'concerned',\n",
       " 'concerning',\n",
       " 'concert',\n",
       " 'concession',\n",
       " 'concierge',\n",
       " 'conclude',\n",
       " 'concluded',\n",
       " 'conclusion',\n",
       " 'concocted',\n",
       " 'concordia',\n",
       " 'concrete',\n",
       " 'concurred',\n",
       " 'concurrently',\n",
       " 'condemn',\n",
       " 'condemned',\n",
       " 'condemning',\n",
       " 'condition',\n",
       " 'conditioning',\n",
       " 'condo',\n",
       " 'condolence',\n",
       " 'conduct',\n",
       " 'conducted',\n",
       " 'conducting',\n",
       " 'coney',\n",
       " 'conference',\n",
       " 'confessed',\n",
       " 'confesses',\n",
       " 'confession',\n",
       " 'confidently',\n",
       " 'confirm',\n",
       " 'confirmation',\n",
       " ...]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Create WordNetLemmatizer object\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "hulett_lemma = []\n",
    "\n",
    "for i in range(len(hulett)):\n",
    "    word = hulett[i]\n",
    "    lemma = wnl.lemmatize(word)\n",
    "    hulett_lemma.append(lemma)\n",
    "\n",
    "hulett_lemma = set(hulett_lemma)\n",
    "\n",
    "sorted(hulett_lemma)\n",
    "\n",
    "\n",
    "\n",
    "# # single word lemmatization examples\n",
    "# list1 = ['kites', 'babies', 'dogs', 'flying', 'smiling',\n",
    "# \t\t'driving', 'died', 'tried', 'feet']\n",
    "# for words in hulett:\n",
    "# \tprint(words + \" ---> \" + )\n",
    "\t\n",
    "#> kites ---> kite\n",
    "#> babies ---> baby\n",
    "#> dogs ---> dog\n",
    "#> flying ---> flying\n",
    "#> smiling ---> smiling\n",
    "#> driving ---> driving\n",
    "#> died ---> died\n",
    "#> tried ---> tried\n",
    "#> feet ---> foot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>biden</th>\n",
       "      <td>223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trump</th>\n",
       "      <td>204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>american</th>\n",
       "      <td>151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>president</th>\n",
       "      <td>135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>democrat</th>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>america</th>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>state</th>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>would</th>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>person</th>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>white</th>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           freq\n",
       "word           \n",
       "biden       223\n",
       "trump       204\n",
       "american    151\n",
       "president   135\n",
       "democrat    123\n",
       "america     102\n",
       "state        80\n",
       "would        80\n",
       "person       79\n",
       "white        78"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hl = df_vocab.all_nonstop_words.to_list()[0]\n",
    "v = vocab_freq(hl, _singularize=True, top_n=10)\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ",.\",\".,-,.,.,-.,,',-.,-.,,-..,.,,.,,\".\",.\".\",,-.,-\n",
      "..,?,,.!-,,?,..,.,.,\",\"..,,.,..:\",,.\"?..,,.\"\":\":..\n",
      "..\";!\n",
      "\n",
      ",:\",,.,..\"!..,,,.:\",,.\".,.:\"(),.\".:\",,--?\"?:\",,,?\"\n",
      ",.,.,.\"\".,.\",\"..-\".\".,-...,..,-:\"...-.\"!,-\"\"..\n",
      "\n",
      ";,..!,./.,,.%..:,,,,,,;.,,,..,....!.,....,,.,..,,,\n",
      ".-..,,,.....,,.\n",
      "\n",
      "-.,,....,-.,,-.,.-.-.\",\"-?.-\"\".-\"\",-..\".\"-,\"\".,:\"?\n",
      "\":\".\",.?-?:.\"\"..,.,.,,,,!.-?:--,\".\",,.,.-,-,.,.().\n",
      "..,.\n",
      "\n",
      ".,??..,:,,,-?:\",,.\"??,?,.\"\".\"\".,,.,,.,.,,-..:,,.(%\n",
      ")-..,,.,-,(),.(,.)..(%),,,:(%).\"\".,.:,,--+.,-.\"\".\n",
      "\n",
      "...,.,,.\"\",,.,,,,.,.,.,-,.,.;....,..,.\"\".\"\",..,.,,\n",
      ",.,-.,\".\",,.,.,,..,.-,-,.:,,?,,\"\"\"\"?.,,?\n",
      "\n",
      ".,/...,...,.-.,-.??.,.,.?,,.:.......,..?..:\",,.\"--\n",
      ",..-,..\",\",.\"\".,:??,--?..\n",
      "\n",
      ".,,..,.......\",\",,..,..,..\"\"..\",\",,,,...,....,-,,-\n",
      "\".\"\".,.\":.\",\"-,,.,,\".\"-,.:,.-,\",\",-,.\n",
      "\n",
      "--....,.:,,,,,,,.,.,,\"\".,.,,.(--),,,,,,,,,,....,.-\n",
      ",?!,.?\"!\"!-\"\".,.,\"\"\"-.\",,.,.,.,\".\":,,.\",\",..\",\"\".\"\n",
      ".!\n",
      "\n",
      ":,\"\"\"\".--,,,.--,.\"\".,,\",,,.\":\".\",..,.\"\";-.,.?.,--,\n",
      ".,-.\"\",\".\":\"..\",,.,-,,,,\"\",!-.-,,,,\"\".-,-\".\",-.\n",
      "\n",
      ".,-.,.,,.\".\"\"-.\",.,,.,,.'.,.,,..,.,%,,.,-.,.,-..,.\n",
      ":\".\",.--?..-,...,\"\".,....,.,\"!\",-:\",.,,..?,,,,?.\"\n",
      "\n",
      "\",,.\"-.,:\",.\",,\"?!,,,.,.\"....,,,.....,,,$.%..,.,.,\n",
      ",.,,$.,,.,,.-..,,,..,,-.,,-,.,\".\"\n",
      "\n",
      "-:?.,,.\",.\",.-..,\".\"-\",\"..:\".,...,-,.\":\"---...,.\"-\n",
      ",,.\".\".,:\".\",:\",:..\",,.?,,.\"\"..?:,\".\".,-..,,,,!\n",
      "\n",
      ",/,.::\",..\"-,,.,,,..,,..\"?\"\"\"\"\"?,-:\",,\"\".\":-...,,\"\n",
      "\"\"\"?!\"?\"\"\",....:---,,-,.!,,,,,,.,.,.-..\n",
      "\n",
      "\"\",,,-.,,.\"\"-.:\".\",,,.!,:\"-!!\",:.,...,,.,-\"\"-,,.\"\"\n",
      "..,...-.,,--,-.,:\"!\",,..,.\"\".\n",
      "\n",
      "-,/,.\"..!\"!,,,.\",!\"-\"\".\"\",,...,-.,,-.,''.-.,.-.,,.\n",
      ".-\"\"?:.:\"-!\"\"\",,,.\"\".,\"\"-..,....!%,,..--...,,.%.?.\n",
      ",?.\n",
      "\n",
      ".,,,.,.:\",.,:,.-,,.,.?\"!--:\"...\"!,.,,,-.,\"\"-..,.,.\n",
      "\".\"-,,,:\"().\"!?\"..\"..,\".\",\".\"!;\"\"\".\"!:.\"\".,\"\"\"\".,.\n",
      ".,,,,,..\n",
      "\n",
      ":\".\":\"!\"!.\"\"',,,,..()..,.,,....()\"\"\".\"()\"\".;.()-..\n",
      ".;.(),-.,\".\"():\".\"()',,,,.(,)\"\",.(,)/.(,)!(,)-,,,.\n",
      "..-,,,,----.,,\"\"\",\".:-.--.,,,,,!\n",
      "\n",
      "\"----\".\",,.,.\":,-,.,-,,.!...,.,.,.,-.,,.,-..,....;\n",
      ".:\",,,.\".,\".\"()-,-,.,,,.(,)\",\"%...-,%.,%\"\".\",\".\"\",\n",
      ",;,,....\n",
      "\n",
      "-,,,..,.,.,.--..,.?,.-.(.)...-..:.:,,.!.!-.,..\"\"..\n",
      "...,..,...\".\",.\n",
      "\n",
      ":\",,().,...()-.,,.\"(\",\".\"\".)\",\",.-.\"\",,,,\"...\",,!-\n",
      "\"\"-,.,.\"-\",\"..,\"?:\",??--,.\",:\".\"?--./.-/.,,/./.\"?\"\n",
      "\"\"-?..\".\"\n",
      "\n",
      "\",\".\"','..\"'.\"\"?,-.-,.-\"\"--.,.\"\"-:...-..-..,..-.\"\"\n",
      "-:-......-,./.,\".\"..-,,...\"\"...,..\".\"-\"\"-,.\"!\"\n",
      "\n",
      "&\"\"-..:\"\"\"-.\",..,....,.,,.$,.-.?-/.,.,-.,,,-....!.\n",
      ",!-..,,,\"!\"\n",
      "\n",
      ".-,,\"\".-,,,..-..\"\"\"\",\".\"\",\"\"\"!-,,\"\"--\"\"\"!\",,\"\".\"\"\"\n",
      ",\"\"\".\"\",\"\"\"\"-.\".\"\"\",.-,!,/.\",\".,.-..\"\".,,-.\n",
      "\n",
      "...-,.-\",\",.\"\"\"\".\"\".\"\"\",,.\"\"\",.\".\"-,,,-,(%),(%).,-\n",
      "\".\",,-..,%.--.,.,,.,,,%,.(%)\".\".,-,(),.,,\".\":\"\".,.\n",
      ",,.,,\"\".\n",
      "\n",
      "...,\",\".\"\".,.,.,-..-,,,...,:),,)..\"\".,.\".\",,\"\".,,,\n",
      "??.:-:\";,..,;,.\"\n",
      "\n",
      ".-,.,,.,.....,-,,.-.\"\"().-,\"\".-.,.,....,.,,-,,,,.-\n",
      ",+,.-..,.,,......,,\"\"....,.,.\n",
      "\n",
      ",-,,,:\"..\":\".\"\"\"\"\"\"\"\".\",-.,\",\".,,..,,:\".\",\"\"\"\".-.,\n",
      ",.,.,:\".,,;.\",\".\".,.-,,,-,,-,-.\"\",-.-,-,-,\"\"-,,,,.\n",
      "\",\"\"\"-,.\n",
      "\n",
      "!.....,.\"\"/-,,,.,-..-,-.,,.,,,,.(.)...,.+-.,...$,,\n",
      "$,.-.,,-.\"\"..:,,,,,,,,,,,,,(,,),,-,!:,,.,\"\"\".\".,,,\n",
      ",-...!\n",
      "\n",
      "\",\"-,:\"',.\"\"\"....\".\".,,.\"\"\"\"-.,..,\".\",,,\"\".\",,\",,\"\n",
      "\".--,-..\".\",..\",\"\".\"-..,,-.,\".\",,!\",\"\n",
      "\n",
      "..,,().!\"\".,,:\",,.',,..\".,,.,\"\".,-,:\",,,,,,,..,,.\"\n",
      ".,,,\"\"\".\",,.,-.,.\"\"??\".\",,,,,.\n",
      "\n",
      ",,.,,.,,..,,,,,,,\"-!\".,,,.,...,.,,..,,.,,..,',.,..\n",
      ".,.,-.,.,,.,-..,.,,..,--\"\".--.\n",
      "\n",
      ":\"\",\"\",\"\",\"\",\".\":.--.%%.-:..:,.:%%.,!--:..-.-,,.-.\n",
      ".,,.!,.\"\"\".\":\"'.\"(.).,--,,,-.\"\"\".\"...\"\"..(-).\",!\"?\n",
      "?,,\".\"!\n",
      "\n",
      ",,,./.--.,,.,-.,,.,,,,.-...,.,.(.)-,,-,,-.,,..,,.,\n",
      ".,...,-.,,,,.;.,\"\",,.,,,-,.,,,,.,.,,.\n",
      "\n",
      ".,-,......,:\".\",,,,,....,..,--..,\"--,\".,,.,,....,;\n",
      ".:,?,()...,-...,.\n",
      "\n",
      ",,,.\".\"\"\"\"\"-.:\"',''.\",!'..,,,-...\"\"%%,%%..,......,\n",
      ",,...?,,-.,.\"\".!,.,-./-?..!.,!,\".\"\n",
      "\n",
      ".,.,.\"\"-\"-.\",-,,,.\"\".\"\"\"\".\"\"?,-.-.,./,..-..,.,,..,\n",
      ".,,.--.,\".\",,.---,,../,-.,,.,.\n",
      "\n",
      ".\"\",..,...,..,....,,.?,-.-?-?-.\"\"...,\",\".,-.$+....\n",
      "\"\"\"\",--,-!?.,.?,.,-,.-,,,,...-...,.\n",
      "\n",
      "\"\"...\"\"\".\"\"\",,\".\",.,\"\"\".\",..'\"\".,.,\",\"\",\"\"\"...\"\"\",\n",
      ",-.\",\"\"\"\".\"-\",\".\"-..,\":,\".,,..\"\"..\".,,,-,,,\".\",,.\"\n",
      "\n",
      "\n",
      "....,..,..,.,,\".\".\",\"'.\"..\"\"-..,,,\".\".,,\"'.,,.,..$\n",
      ".,..',..,....:??,,??,!\n",
      "\n",
      ",:\".\"\",.,.,,.,.\".,,.-,..,.,.-.:,.\"\",.,\",\".\",\"\".\".-\n",
      ".,,-,.\".\":\"-.\".-..,:\"\".-.-.\".\",,.\n",
      "\n",
      ",...-.,.,,\"\"\".\"...,.,\"\"\".\"-.,,,\"\"..--,,..,..,,.,.,\n",
      "\".\"\"\"\".\"-....-.-,,,-.\n",
      "\n",
      "\"\"--.,-,,.!,.-,,.\":,,\"\":,,\"-.-.,.,,-!,,,,,,.,-,,-.\n",
      "..:?,,'-.,\"-\"...?,,,,,.\n",
      "\n",
      ".-.,,....,..,.,-.,./.?,.,-.,,.-...,-.(---)?,..,,,,\n",
      "-.!,-,%-,--,-,,.,\".\",.....,..,,--.\n",
      "\n",
      "..\",\".,.,,.,/,.-.,..\".\",-.,/,.,,.-..,..\"\"\"\"\"\".,\"\"!\n",
      "(!).,,.,.--,.,-,..?,,.\"\"?,,,.\"?\",.,?\n",
      "\n",
      ",,...,,,--...-.-.$.-,..,,,.,,..%.%,,%.,,.,..-,.--.\n",
      "\"\".\".\"..-?.,,.\n",
      "\n",
      "\"-,.,.\":\",..,..,..-.\".()--.,..-,,.:\"..?.\".\",\"..().\n",
      "\"\"\".\",.\"\"$..,,.,,-..---..:\"-...,-,!\"-,,,,,,,-.,...\n",
      ",,.\n",
      "\n",
      "\".\"\"\"\"\"\".\",\"\"\"\".\"\"..!,--..,,,,,,.,,.-,,,,,..-..,,.\n",
      "..,,-,...$.\"\".,....---..,-?\"!\"\n",
      "\n",
      ",..-,,,.,..---..,-..,.-,,..,,..\"\",-.,.,,,.,.,.\"\".,\n",
      "'.,.\"\"......,.,.,.,,?\n",
      "\n",
      "....,(,,,),.,.,,,,.,.,,,.(:\"\"\"!\"),-.,:\",-.\"...,..,\n",
      ".,-.,\"--\".\"\".,\"\"...,\"\",,.,,.,,\",\",,.\"\".\n",
      "\n",
      ",,\"\".-....,,:\",...,():.\":\".,,-,.-,-...,.\"-\"\",..,,.\n",
      "...,\".\"-..(\"\").\"\".%.%'-,.%,(),..-,..-.,,,,.\n",
      "\n",
      ",,.-:\",.-,--.\":\",.\"..,,,,,\"\".:\",,.,..\",\",\"\",\"\",\".,\n",
      ",()$,().'.,.,..,.,,\".,.\",,\"\"\".\".\",\"..\n",
      "\n",
      "\"\",...,.---,,...\"\"\",\"\"\"\",\".\",\",,..\"\",,,.:\",.\",,\"\"\"\n",
      "?\"-\".\":\".\"..,,..,,,,,,.,,.\":.%:.\"'.\"\":\",\"-,.\n",
      "\n",
      "..(,,,,,).-.:\",,-.\"\"\",.\"\",\"\"!...-...\"\"-..,-.,.,,,.\n",
      "\",\",,.,.,.'-.,/,.\"\".,-\".\".\"\"..,,\"\"\"!\",-,,.,?\n",
      "\n",
      ",-.\":..\"..,.,..,,,..,-,.,-.-.-...,.,.,..-,....,,,,\n",
      ".,..\n",
      "\n",
      "-,.\"\"\".\"....\"\".,,-.,,-.\"\"(!).,,,,,,,,,-.,,..,..,\"&\n",
      ",\".,-.,,.,!,',..,:.\n",
      "\n",
      "%..\"\".-\",\"-..-.\"\"...,\",\".-.,---.-..,.\".,\"..-...,,.\n",
      ".\"\".-,,...:,,,,,,-;,-.,-.,.,.\".\"(.)!.\"\"!\n",
      "\n",
      "-:\".\",.-..,.-,.,\"\",,..,.,..,.-,?..,,\"--\",,-.,-,.,/\n",
      "-.,?,.,,-,-,--,,-.-..,\"\"\",,.\"\n",
      "\n",
      ".,,.-.,..,,..-?\"!\",,,.\",\".\",..\"(.!)..,-,...,\".-.\",\n",
      ",,-,.,,.-.,..,,,?.-.,-',..,?\n",
      "\n",
      ".-.-.,:\"!\",..,.,,\"',,,.\",..,\"\".,,,-,-.\"\"..,.\".\".-.\n",
      ",.,\".\"!,..,.!?!.!??,,...,,.,.!-...,---\".\"\n",
      "\n",
      ",\"\"\",,.\".,,.?,,,,;.\"-\",-,,.,,!?....,-.?--,.,,.--,-\n",
      ".?!,,,,-,..,.,-.-.-,.!--,.,,,-,-.,,,,.,\"%\".\"\"..,\",\n",
      ",,,.\"\n",
      "\n",
      ",\",.\"!..:\"-..\",,,.:\":?-?\",\",,,,-..,,.\",\"\",-\"-\",?..\n",
      ",.,\"\"-\"?\"\"\"\"\"\"\"?,,.,,.,,.,,-..\"\"\"\",,.,,.,,,.,,.!.\n",
      "\n",
      "\"?\",.,.,-.--..-\",\",\",\".-:\"..\",!,:\"..,,....\":\"?\".--\n",
      ".,,--.:\",.\":\"....\".,,,,.,,,-.\n",
      "\n",
      ",.,.-,,,\".\",,.,.,,%,\"\".%.\"-,.\".\";.\".\".\",\"\".,-?:\"..\n",
      ".\"...-,\"\"-.,,.,.\n",
      "\n",
      ",:,-,.,.,,,-.\"-\",.,.-.\"\".!,.,--,-,,--,.-,,..-..\"\".\n",
      ",-.-,,,.,.--..,\"\".-,.-.,-\"\"-,,-.,-,.,,.,:.,,.\n",
      "\n",
      ".,.,,.-(!),..-.,,.:\",-,.\",..\"?\"(-:,,,,,-,!)\"-\",--,\n",
      ",\"\"\"-.\",-.\"-\".,\"-\"--.,\"-\".,-.-.,..,,.,\"\"\".\"..,,.\n",
      "\n",
      ".-?,!,,,...,,,...,..,,.....(,,\"\".!).,...!,.,.,.,..\n",
      ".,!,..-,\",\"',...,-.!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(df)):\n",
    "    print(stack_string(df['story_punctuation'][i], 50))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7e4608ffab33544aecd2f97721d910c9b0c38b0c6fa6ab7667a80c134f821554"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
